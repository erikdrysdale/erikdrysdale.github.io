{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d9b88a",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "## (1) Background\n",
    "\n",
    "This notebook will show how to tune the $\\gamma$ and $\\rho$ hyperparameters using 10-fold CV so that we are able to fit a model with the best shot of obtaining high performance on a test set. Recall that the regularized loss function being minimized for the parametric survival models are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "-\\ell(\\alpha, \\beta; t, d, X) + \\gamma\\big(\\rho \\tilde{\\| \\beta_{1:} \\|_1} + 0.5(1-\\rho)\\|\\beta_{1:}\\|_2^2\\big), \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where the first term is the (negative) data log-likelihood and the second term is the elastic net penalty. Note that i) there is a tilde over the L1-norm because a smooth convex approximation is used in the actual optimization, and ii) the $\\beta$ (aka scale) parameters ignore index zero which is by tradition treated as an intercept and is therefore not regularized (i.e. $\\beta_0$ and $\\alpha$ are unregularized). Higher values of $\\gamma$ will encourage the L1/L2 norm of the coefficient to be smaller, where the relative weight between these two norms is governed by $\\alpha$. For a given level of $\\gamma$, a higher value of $\\rho$ will (on average) encourage more sparsity (aka coefficients that are exactlty zero).\n",
    "\n",
    "## (2) Performing a grid-search \n",
    "\n",
    "For a given value of $\\rho>0$, there are a sequence of $\\gamma$ from zero (or close to close) to $\\gamma^{\\text{max}}$ that yield the \"solution path\" of the elastic net model. $\\gamma^{\\text{max}}$ is the infimum of gamma's that achieve 100% sparsity (i.e. all coefficients other than the shape/intercept scale are zero). In practice, it is efficient to start with the highest value of $\\gamma$ and solve is descending order so that we can initialize the optimization routine with an initial value of a less complex model.\n",
    "\n",
    "The rest of this notebook will how to do 10-fold CV to find an \"optimal\" $\\gamma$/$\\rho$ combination on the [colon](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/colon.html) dataset using the `SurvSet` package.\n",
    "\n",
    "The first block of code loads the data, and prepares an `sklearn` `ColumnTransformer` class to impute missing values for the continuous values and do one-hot-encoding for the categorical ones. Because the `parametric` class (as a default) learns another standard scaler, the design matrix will be mean zero and variance one during training (which is important so as to not bias the selection of certain covariates in the presence of regularization). The `SurvSet` package is structured so that numeric/categorical columns always have a \"num_\"/\"fac_\" prefix which can be singled out with the `make_column_selector` class. Lastly we do an 80/20 split of training/test data which is stratified by the censoring indicator so that our training/test results are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da78f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from SurvSet.data import SurvLoader\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sksurv.metrics import concordance_index_censored as concordance\n",
    "from paranet.models import parametric\n",
    "from paranet.utils import dist_valid\n",
    "\n",
    "\n",
    "# (i) Load the colon dataset\n",
    "loader = SurvLoader()\n",
    "df, ref = loader.load_dataset('colon').values()\n",
    "t_raw, d_raw = df['time'].values, df['event'].values\n",
    "df.drop(columns=['pid','time','event'], inplace=True)\n",
    "\n",
    "# (ii) Perpeare the encoding class\n",
    "enc_fac = Pipeline(steps=[('ohe', OneHotEncoder(sparse=False, drop=None, handle_unknown='ignore'))])\n",
    "sel_fac = make_column_selector(pattern='^fac\\\\_')\n",
    "enc_num = Pipeline(steps=[('impute', SimpleImputer(strategy='median'))])\n",
    "sel_num = make_column_selector(pattern='^num\\\\_')\n",
    "enc_df = ColumnTransformer(transformers=[('ohe', enc_fac, sel_fac),('num', enc_num, sel_num)])\n",
    "\n",
    "# (iii) Split into a training and test set\n",
    "frac_test, seed = 0.2, 1\n",
    "t_train, t_test, d_train, d_test, x_train, x_test = train_test_split(t_raw, d_raw, df, test_size=frac_test, random_state=seed, stratify=d_raw)\n",
    "rho_seq = np.arange(0.2, 1.01, 0.2).round(2)\n",
    "n_gamma = 50\n",
    "gamma_frac = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fabea4",
   "metadata": {},
   "source": [
    "To do cross-validation, we split the training data into a futher 10 folds and fit a parametric model on 9 out of 10 of the folds, and make a prediction on the 10th fold and measure performance in terms of [Harrell's C-index](https://jamanetwork.com/journals/jama/article-abstract/372568) (aka concordance). Thus, for every fold, $\\rho$, $\\gamma$, and distribution, there will be an average out-of-fold c-index measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6be4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal dist=exponential, rho=1.0, gamma (index)=9\n"
     ]
    }
   ],
   "source": [
    "# Load modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from SurvSet.data import SurvLoader\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sksurv.metrics import concordance_index_censored as concordance\n",
    "from paranet.models import parametric\n",
    "from paranet.utils import dist_valid\n",
    "\n",
    "\n",
    "# (i) Load the colon dataset\n",
    "# https://stat.ethz.ch/R-manual/R-devel/library/survival/html/colon.html\n",
    "loader = SurvLoader()\n",
    "df, ref = loader.load_dataset('colon').values()\n",
    "t_raw, d_raw = df['time'].values, df['event'].values\n",
    "df.drop(columns=['pid','time','event'], inplace=True)\n",
    "\n",
    "# (ii) Perpeare the encoding class\n",
    "enc_fac = Pipeline(steps=[('ohe', OneHotEncoder(sparse=False, drop=None, handle_unknown='ignore'))])\n",
    "sel_fac = make_column_selector(pattern='^fac\\\\_')\n",
    "enc_num = Pipeline(steps=[('impute', SimpleImputer(strategy='median'))])\n",
    "sel_num = make_column_selector(pattern='^num\\\\_')\n",
    "enc_df = ColumnTransformer(transformers=[('ohe', enc_fac, sel_fac),('num', enc_num, sel_num)])\n",
    "\n",
    "# (iii) Split into a training and test set\n",
    "frac_test, seed = 0.2, 1\n",
    "t_train, t_test, d_train, d_test, x_train, x_test = train_test_split(t_raw, d_raw, df, test_size=frac_test, random_state=seed, stratify=d_raw)\n",
    "rho_seq = np.arange(0.2, 1.01, 0.2).round(2)\n",
    "n_gamma = 50\n",
    "gamma_frac = 0.001\n",
    "\n",
    "# (iv) Make \"out of fold\" predictions\n",
    "n_folds = 10\n",
    "skf_train = StratifiedKFold(n_splits=n_folds, random_state=seed, shuffle=True)\n",
    "holder_paranet, holder_cox = [], []\n",
    "for i, (train_idx, val_idx) in enumerate(skf_train.split(t_train, d_train)):\n",
    "    # Split Training data into \"fit\" and \"val\" splits\n",
    "    t_fit, d_fit, x_fit = t_train[train_idx], d_train[train_idx], x_train.iloc[train_idx]\n",
    "    t_val, d_val, x_val = t_train[val_idx], d_train[val_idx], x_train.iloc[val_idx]\n",
    "    # Learn data encoding and one-encoded matrices\n",
    "    enc_fold = enc_df.fit(x_fit)\n",
    "    mat_fit, mat_val = enc_fold.transform(x_fit), enc_fold.transform(x_val)\n",
    "    t_hazard_val = np.repeat(np.median(t_fit), mat_val.shape[0])    \n",
    "\n",
    "    # Fit model\n",
    "    mdl_para = parametric(dist_valid, mat_fit, t_fit, d_fit, scale_t=True, scale_x=True)\n",
    "    # Fit along the solution path\n",
    "    holder_fold = []\n",
    "    for rho in rho_seq:\n",
    "        gamma_mat, thresh = mdl_para.find_lambda_max(mat_fit, t_fit, d_fit, rho=rho)\n",
    "        p = gamma_mat.shape[0]\n",
    "        gamma_max = gamma_mat.max(0)\n",
    "        gamma_mat = np.exp(np.linspace(np.log(gamma_max), np.log(gamma_frac*gamma_max), n_gamma-1))        \n",
    "        gamma_mat = np.vstack([gamma_mat, np.zeros(gamma_mat.shape[1])])\n",
    "        for j in range(n_gamma):\n",
    "            gamma_j = np.tile(gamma_mat[[j]], [p,1])\n",
    "            init_j = np.vstack([mdl_para.alpha, mdl_para.beta])\n",
    "            mdl_para.fit(mat_fit, t_fit, d_fit, gamma_j, rho, thresh, alpha_beta_init=init_j, grad_tol=0.05)\n",
    "            # Make out of sample predictions\n",
    "            res_rho_j = pd.DataFrame(mdl_para.hazard(t_hazard_val,mat_val), columns=dist_valid)\n",
    "            res_rho_j = res_rho_j.assign(d=d_val, t=t_val, j=j, rho=rho)\n",
    "            holder_fold.append(res_rho_j)\n",
    "    # Merge and calculate concordance\n",
    "    res_rho = pd.concat(holder_fold).melt(['rho','j','t','d'],dist_valid,'dist','hazard')\n",
    "    res_rho['d'] = res_rho['d'] == 1\n",
    "    res_rho = res_rho.groupby(['dist','rho','j']).apply(lambda x: concordance(x['d'], x['t'], x['hazard'])[0]).reset_index()\n",
    "    res_rho = res_rho.rename(columns={0:'conc'}).assign(fold=i)\n",
    "    holder_paranet.append(res_rho)\n",
    "# Find the best shrinkage combination\n",
    "res_cv = pd.concat(holder_paranet).reset_index(drop=True)\n",
    "res_cv = res_cv.groupby(['dist','rho','j'])['conc'].mean().reset_index()\n",
    "param_cv = res_cv.query('conc == conc.max()').sort_values('rho',ascending=False).head(1)\n",
    "dist_star, rho_star, j_star = param_cv[['dist','rho','j']].values.flat\n",
    "j_star = int(j_star)\n",
    "idx_dist_star = np.where([dist_star in dist for dist in dist_valid])[0][0]\n",
    "print(f'Optimal dist={dist_star}, rho={rho}, gamma (index)={j_star}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b6e8d9",
   "metadata": {},
   "source": [
    "Because each 9/10 fold combination will have a different $\\gamma^{\\text{max}}$, we create distribtuion specific sequence (of length 50) down to $0.001\\cdot\\gamma^{\\text{max}}$ in a log-linear fashion which tends to yield a more linear descrease in the decrease in sparsity. Thus when we when find $j^{*}$, what we are finding is the index of this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3654f54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of smaple conconcordance: 66.7%\n"
     ]
    }
   ],
   "source": [
    "# (v) Fit model with the \"optimal\" parameter\n",
    "enc_train = enc_df.fit(x_train)    \n",
    "mat_train, mat_test = enc_df.transform(x_train), enc_df.transform(x_test)\n",
    "mdl_para = parametric(dist_valid, mat_train, t_train, d_train, scale_t=True, scale_x=True)\n",
    "# Find the optimal gamma\n",
    "gamma, thresh = mdl_para.find_lambda_max()\n",
    "gamma_max = gamma.max(0)\n",
    "gamma_star = np.exp(np.linspace(np.log(gamma_max), np.log(gamma_frac*gamma_max), n_gamma-1))\n",
    "gamma_star = np.vstack([gamma_star, np.zeros(gamma_star.shape[1])])\n",
    "gamma_star = gamma_star[j_star,idx_dist_star]\n",
    "# Re-fit model\n",
    "mdl_para.fit(gamma=gamma_star, rho=rho_star, beta_thresh=thresh)\n",
    "\n",
    "# (vii) Make predictions on test set\n",
    "t_haz_test = np.repeat(np.median(t_train), len(t_test))\n",
    "hazhat_para = mdl_para.hazard(t_haz_test, mat_test)[:,idx_dist_star]\n",
    "conc_para = concordance(d_test==1, t_test, hazhat_para)[0]\n",
    "print(f'Out of smaple conconcordance: {conc_para*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae93bc8",
   "metadata": {},
   "source": [
    "Lastly, we make a prediction on the test set and find that we obtain a c-index score of 66.7%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('paranet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb9dd58d514330c7fa95404a6cb6eb8b6c69056f9aa4b8786fa94908a34d85b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
