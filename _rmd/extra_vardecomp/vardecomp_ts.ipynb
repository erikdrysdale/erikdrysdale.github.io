{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individualized models as a function of variance decomposition\n",
    "\n",
    "Assume each individual time series follows a stationary AR model:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_{i,t} &= \\alpha_i + \\phi y_{i,t-1} + e_i, \\hspace{2mm} |\\phi| < 1, \\hspace{2mm} e_i \\sim N(0,1) \\\\\n",
    "&= \\sum_{j=0}^t \\phi^{t-j}\\big( \\alpha + e_j\\big)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The expected value ($\\lim_{n \\to \\infty}$) and the variance are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E[y_{i,t}] &= \\frac{\\alpha_i}{1-\\phi} \\\\\n",
    "Var[y_{i,t}] &= \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import plotnine\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N=100;K=10;alpha=0;phi=0.25\n",
    "def dgp_panel(N,K,alpha,phi):\n",
    "    Alpha = np.repeat(alpha, K).reshape([1,K])\n",
    "    Emat = np.random.randn(N+1,K)\n",
    "    Ymat = np.zeros([N+1,K])\n",
    "    Ymat[0] = Alpha + Emat[0]\n",
    "    for i in range(1, N+1):\n",
    "        Ymat[i] = Alpha + phi*Ymat[i-1] + Emat[i]\n",
    "    return Ymat[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.246806648839502\n"
     ]
    }
   ],
   "source": [
    "phi = 0.75; N=10\n",
    "Ysamp = dgp_panel(N=N, K=100000, alpha=0, phi=phi)\n",
    "print(np.sum(Ysamp**2,0).mean())\n",
    "# print(np.sum((Ysamp-Ysamp.mean(0))**2,0).mean())\n",
    "# Ysamp.var(0,ddof=0).mean()*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=100;K=1000\n",
    "Xmat = np.random.randn(N,K)\n",
    "Amat = 0*np.random.randn(1,K)\n",
    "Ymat = Xmat+Amat\n",
    "Xmat.var().mean()/Ymat.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9896999923382773"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "GENERATE AN INDIVIDUAL TIME SERIES\n",
    "\"\"\"\n",
    "def dgp_ts(n,alpha,phi):\n",
    "    e = np.random.randn(n+1)  # Need to create a t=0\n",
    "    y = np.zeros(n+1)\n",
    "    y[0] = alpha + e[0]  # Unfortunately we cannot vectorize\n",
    "    for i in range(1, n+1):\n",
    "        y[i] = alpha + phi*y[i-1] + e[i]\n",
    "    return y[1:]\n",
    "# # Notice mean is alpha/(1-phi)\n",
    "# np.random.seed(1)\n",
    "# n = 1000\n",
    "# plotnine.options.figure_size = (4, 3)\n",
    "# alpha=2; phi=0.72\n",
    "# (ggplot(pd.DataFrame({'y':dgp_ts(n, alpha, phi),'idx':list(range(n))}),aes(x='idx',y='y')) + \n",
    "# theme_bw() + geom_line() + geom_hline(yintercept=alpha/(1-phi),color='red'))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "GENERATE A GROUP OF TIME SERIES: y_{it} = alpha + phi*y_{it-1} + e\n",
    "k: # of individuals\n",
    "sd_alpha: how much variation to draw around alpha ~ N(0, sd_alpha^2)\n",
    "phi: the AR coefficients\n",
    "\"\"\"\n",
    "def gen_group(nmin, nmax, k, sd_alpha, phi):\n",
    "    n_seq = np.random.randint(nmin, nmax, k)\n",
    "    alpha_seq = 0 + sd_alpha*np.random.randn(k)\n",
    "    holder = []\n",
    "    for i, n in enumerate(n_seq):\n",
    "        holder.append(pd.DataFrame({'idt':i, 'y':dgp_ts(n, alpha_seq[i], phi)}))\n",
    "    df = pd.concat(holder).reset_index(None, True).assign(varname='y')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://biostat.duke.edu/sites/biostat.duke.edu/files/Longitudinal%20Data%20Analysis%20-%20RM%20ANOVA.pdf\n",
    "nmin, nmax, k, sd_alpha, phi = 25, 26, 25, 0, 0.5\n",
    "def rm_anova(nmin, nmax, k, sd_alpha, phi):\n",
    "    df = gen_group(nmin, nmax, k, sd_alpha, phi)\n",
    "    df['time'] = df.groupby('idt').cumcount()+1\n",
    "    # Total variation\n",
    "    ybar = df.y.mean()\n",
    "    tss = df.y.var()*(len(df.y)-1)\n",
    "    # Calculate the variation over the time points\n",
    "    df_sst = df.groupby('time').y.mean().reset_index().rename(columns={'y':'ybar_time'})\n",
    "    df_sst = df_sst.merge(df.groupby('time').size().reset_index().rename(columns={0:'n_time'}))\n",
    "    sst = np.sum(df_sst.n_time*(df_sst.ybar_time - ybar)**2)\n",
    "    # Calculate the variation within the individuals\n",
    "    df_sss = df.groupby('idt').y.mean().reset_index().rename(columns={'y':'ybar_idt'})\n",
    "    df_sss = df_sss.merge(df.groupby('idt').size().reset_index().rename(columns={0:'n_idt'}))\n",
    "    sss = np.sum(df_sss.n_idt*(df_sss.ybar_idt - ybar)**2)\n",
    "    # Calculate the variation over the residuals\n",
    "    ssr = tss - (sss + sst)\n",
    "    return sst, sss, ssr\n",
    "\n",
    "sim = np.array([rm_anova(nmin, nmax, k, sd_alpha, phi) for z in range(1500)])\n",
    "sim = pd.DataFrame(sim, columns=['sst', 'sss', 'ssr'])\n",
    "sim.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "()/phi**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.sss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ssr = df.merge(df_sst,'left','time').merge(df_sss,'left','idt')\n",
    "# ssr = np.sum( (df_ssr.y - df_ssr.ybar_idt - df_ssr.ybar_time + ybar)**2 )\n",
    "# print('TSS: %0.1f, SST: %0.1f, SSS: %0.1f, SSR: %0.1f' % (tss,sst,sss,ssr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum( (df_ssr.y - df_ssr.ybar_idt - df_ssr.ybar_time + ybar)**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(((np.random.randn(100)-np.random.randn(100).mean())**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.SSw.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.chi2.rvs(df=tmp.dof_w[0],size=1000).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_w = df.copy().groupby([cn_vv,cn_gg]).apply(lambda x: \n",
    "       pd.Series({'SSw':np.sum((x[cn_val]-x[cn_val].mean())**2)})).reset_index()\n",
    "res_w = res_w.groupby(cn_vv).SSw.sum().reset_index()\n",
    "# (ii) Calculate the between group sum of squares\n",
    "res_b = df.copy().groupby([cn_vv,cn_gg]).apply(lambda x: \n",
    "                 pd.Series({'xbar':x[cn_val].mean(),'n':x[cn_val].shape[0]})).reset_index()\n",
    "res_b = res_b.merge(df.groupby(cn_vv)[cn_val].mean().reset_index().rename(columns={cn_val:'mu'}))\n",
    "res_b = res_b.assign(SSb=lambda x: x.n*(x.xbar - x.mu)**2).groupby(cn_vv).SSb.sum().reset_index()\n",
    "# (iii) Ensure it lines up (SStot == 0)\n",
    "res_tot = res_w.merge(res_b).assign(SStot=lambda x: x.SSw+x.SSb)\n",
    "# (iv) Under null of no difference between groups, should have an F-distribution after DoF adjustment\n",
    "res_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = \n",
    "df.groupby(cn_vv).apply(lambda x: pd.Series({'n':x.shape[0], 'k':x[cn_gg].unique().shape[0]})).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_var(df, cn_gg, cn_vv, cn_val):\n",
    "    # (i) Calculate the within group sum of squares\n",
    "    res_w = df.copy().groupby([cn_vv,cn_gg]).apply(lambda x: \n",
    "       pd.Series({'SSw':np.sum((x[cn_val]-x[cn_val].mean())**2)})).reset_index()\n",
    "    res_w = res_w.groupby(cn_vv).SSw.sum().reset_index()\n",
    "    # (ii) Calculate the between group sum of squares\n",
    "    res_b = df.copy().groupby([cn_vv,cn_gg]).apply(lambda x: \n",
    "                     pd.Series({'xbar':x[cn_val].mean(),'n':x[cn_val].shape[0]})).reset_index()\n",
    "    res_b = res_b.merge(df.groupby(cn_vv)[cn_val].mean().reset_index().rename(columns={cn_val:'mu'}))\n",
    "    res_b = res_b.assign(SSb=lambda x: x.n*(x.xbar - x.mu)**2).groupby(cn_vv).SSb.sum().reset_index()\n",
    "    # (iii) Ensure it lines up (SStot == 0)\n",
    "    res_tot = res_w.merge(res_b).assign(SStot=lambda x: x.SSw+x.SSb)\n",
    "    # (iv) Under null of no difference between groups, should have an F-distribution after DoF adjustment\n",
    "    tmp = df.groupby(cn_vv).apply(lambda x: pd.Series({'n':x.shape[0], 'k':x[cn_gg].unique().shape[0]})).reset_index()\n",
    "    res_tot = res_tot.merge(tmp,'left',cn_vv)\n",
    "    res_tot = res_tot.assign(dof_b = lambda x: x.k - 1, dof_w=lambda x: x.n-x.k)\n",
    "    res_tot = res_tot.assign(Fstat=lambda x: (x.SSb/x.dof_b)/(x.SSw/x.dof_w))\n",
    "    res_tot['pval'] = 1 - stats.f.cdf(res_tot.Fstat, res_tot.dof_b, res_tot.dof_w)\n",
    "    res_tot['gg'] = cn_gg\n",
    "    return res_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
