{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Background\n",
    "\n",
    "Consider the sum of a normal $Z_1 \\sim N(\\mu_1,\\tau_1^2)$ and truncated normal variable and $Z_2 \\sim TN(\\mu_2,\\tau_2^2,a,b)$, $W=Z_1+Z_2$. How can this normal-truncated sum (NTS) distribution be characterized? One approach for doing inference on $W$ is simply to sample from it, as sampling algorithms for both $Z_1$ and $Z_2$ are found in most statistical software. Having an analytical or numerical method for calculating the density (PDF) and cumulative distribution (CDF) functions for $W$ has several approaches over the sampling approach including vectorization, exact solutions, and reproducibility across software programs. Furthermore, sampling methods become increasingly inefficient as one queries the tails or the distribution. The goal of this post is show how to 1) calculate the PDF and CDF for $W$ with `python`, and 2) provide three statistical use cases: i) quality control, ii) two-stage hypothesis testing, and iii) data carving for selective inference. \n",
    "\n",
    "This post will make extensive use of the theoretical results from [Arnold et. al (1993)](https://link.springer.com/article/10.1007/BF02294652) (hereafter Arnold) and [Kim (2006)](https://www.kss.or.kr/jounalDown.php?IDX=831) (hereafter Kim).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Integrating a bivariate normal CDF\n",
    "\n",
    "The CDF of a bivarate normal (BVN) will turn out to be necessary to calculate the CDF of a NTS. While there is no closed-form solution to the CDF of a BVN (unless it's from the origin), the integration problem can be quickly solved by numerical integration method. Additionally, if the user is willing to accept a non-exact solution, there are approximation methods which yield an analytical solution that can be easily vectorized. This section will briefly review both approaches. \n",
    "\n",
    "Following the notation of the literature, the orthant probability of a standard BVN[[1]] $L(h,k,\\rho)=P(X_1\\geq h, X_2\\geq k)$, $h,k \\geq 0$, is equivalent to solving the following integral introduced by [Sheppard](https://royalsocietypublishing.org/doi/10.1098/rsta.1899.0003) 130 years ago:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(h,k,\\rho) &= \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_h^\\infty \\int_k^\\infty \\exp \\Bigg[ - \\frac{x^2-2\\rho x y + y^2}{2(1-\\rho^2)} \\Bigg] dx dy  \\\\\n",
    "&= \\frac{1}{2\\pi} \\int_{\\arccos(\\rho)}^{\\pi} \\exp \\Bigg[ - \\frac{h^2+k^2-2hk\\cos(\\theta)}{2\\sin^2(\\theta)} \\Bigg] d\\theta \\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:sheppard}\n",
    "\n",
    "Where the orthant probability can easily be related to the CDF: $F(h,k,\\rho)=P(X_1\\leq h, X_2\\leq k) = L(h,k,\\rho) + \\Phi(h) + \\Phi(k) - 1$. Note that $\\Phi(\\cdot)$ is the standard normal CDF  (and $\\phi(\\cdot)$ is the PDF). What's interesting about $\\eqref{eq:sheppard}$ is that problem of integrating out both $X_1$ and $X_2$ has been reduced to a single variable of integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical methods can be used to solve $\\eqref{eq:sheppard}$ quickly. The `scipy.integrate.quad` function uses Fortran's QUADPACK under the hood. It will yield identical results to that of the `scipy.stats.multivariate_normal.cdf` function. [Cox 1991](https://www.jstor.org/stable/1403446) showed that the estimate of $L(\\cdot)$ could be approximated using a simple Taylor expansion.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(h,k,\\rho) &= P(X_1 \\geq h, X_2 \\geq k) = P(X_1 \\geq h) P(X_2 \\geq k | X_1 \\geq h) \\\\\n",
    "&= \\Phi(-h) E\\Bigg[ \\Phi\\Bigg(\\frac{\\rho X_1 - k}{\\sqrt{1-\\rho^2}} \\Bigg) \\Bigg| X_1 \\geq h \\Bigg] \\\\\n",
    "&\\approx \\Phi(-h) \\Phi\\Bigg( \\frac{\\rho\\phi(h)/\\Phi(-h)-k}{\\sqrt{1-\\rho^2}} \\Bigg) \\tag{2}\n",
    "\\end{align} \n",
    "$$\n",
    "\\label{eq:cox}\n",
    "\n",
    "Cox showed that this approximation method works well for reasonable ranges of $\\rho$ (absolute coefficient less than 90%).  For users willing to trade off accuracy for speed, $\\eqref{eq:cox}$ allows for rapid vectorization cross different values of $\\rho$, $h$, or $k$. The code block below will define a `BVN` class that can be used to calculate the orthant probability and perform sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>orthant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scipy</td>\n",
       "      <td>0.040615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sheppard</td>\n",
       "      <td>0.040615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cox</td>\n",
       "      <td>0.040670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rvs</td>\n",
       "      <td>0.040710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     method   orthant\n",
       "0     scipy  0.040615\n",
       "1  sheppard  0.040615\n",
       "2       cox  0.040670\n",
       "3       rvs  0.040710"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, truncnorm\n",
    "from scipy.stats import multivariate_normal as MVN\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "class BVN():\n",
    "    def __init__(self, mu, sigma, rho):\n",
    "        assert mu.shape[0]==sigma.shape[0]==2\n",
    "        assert np.abs(rho) <= 1\n",
    "        self.mu = mu.reshape([1,2])\n",
    "        self.sigma = sigma.flatten()\n",
    "        od = rho*np.sqrt(sigma.prod())\n",
    "        self.rho = rho\n",
    "        self.Sigma = np.array([[sigma[0],od],[od, sigma[1]]])\n",
    "        self.A = cholesky(self.Sigma) # A.T.dot(A) = Sigma\n",
    "\n",
    "    def rvs(self, size, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        X = np.random.randn(size,2)\n",
    "        Z = self.A.T.dot(X.T).T + self.mu\n",
    "        return Z\n",
    "\n",
    "    def imills(self, a):\n",
    "        return norm.pdf(a)/norm.cdf(-a)\n",
    "\n",
    "    def sheppard(self, theta, h, k):\n",
    "        return (1/(2*np.pi))*np.exp(-0.5*(h**2+k**2-2*h*k*np.cos(theta))/(np.sin(theta)**2))\n",
    "\n",
    "    def orthant(self, h, k, method='scipy'):\n",
    "        assert method in ['scipy','cox','sheppard']\n",
    "        if isinstance(h,int) or isinstance(h, float):\n",
    "            h, k = np.array([h]), np.array([k])\n",
    "        else:\n",
    "            assert isinstance(h,np.ndarray) and isinstance(k,np.ndarray)\n",
    "        assert len(h) == len(k)\n",
    "        assert np.all(h >= 0) and np.all(k >= 0)\n",
    "        Y = (np.c_[h, k] - self.mu)/np.sqrt(self.sigma)\n",
    "        Y1, Y2 = Y[:,0], Y[:,1]\n",
    "\n",
    "        if method == 'scipy':\n",
    "            sp_bvn = MVN([0, 0],[[1,self.rho],[self.rho,1]])\n",
    "            pval = 1+sp_bvn.cdf(Y)-(norm.cdf(Y1)+norm.cdf(Y2))\n",
    "            return pval \n",
    "        \n",
    "        if method == 'cox':\n",
    "            mu_a = self.imills(Y1)\n",
    "            root = np.sqrt(1-self.rho**2)\n",
    "            xi = (self.rho * mu_a - Y2) / root\n",
    "            pval = norm.cdf(-Y1) * norm.cdf(xi)\n",
    "            return pval\n",
    "\n",
    "        if method == 'sheppard':\n",
    "            pval = [quad(self.sheppard, np.arccos(self.rho), np.pi, args=(y1,y2))[0] for y1, y2 in zip(Y1,Y2)]\n",
    "            return pval\n",
    "\n",
    "mu = np.array([1,2])\n",
    "sigma = np.array([0.5,2])\n",
    "dist_BVN = BVN(mu,sigma,rho=0.4)\n",
    "h, k = 2, 3\n",
    "pval_scipy = dist_BVN.orthant(h, k, method='scipy')[0]\n",
    "pval_sheppard = dist_BVN.orthant(h, k, method='sheppard')[0]\n",
    "pval_cox = dist_BVN.orthant(h, k, method='cox')[0]\n",
    "nsim = 1000000\n",
    "pval_rvs = dist_BVN.rvs(nsim)\n",
    "pval_rvs = pval_rvs[(pval_rvs[:,0]>h) & (pval_rvs[:,1]>k)].shape[0]/nsim\n",
    "methods = ['scipy','sheppard','cox','rvs']\n",
    "pvals = [pval_scipy, pval_sheppard, pval_cox, pval_rvs]\n",
    "pd.DataFrame({'method':methods,'orthant':pvals})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output results are identical using either the `multivariate_normal` class or directly calling integration solvers.  Figure 1A below shows that the Cox-method is very close to the numerical solution, although there are no definitive error bounds (in Cox's paper he found that the worst-case empirical error was around 10%). Although the figure is not shown, there is no improvement using the `sheppard` method with `BVN` over `scipy`'s built in MVN distribution for the run-time for a single calculation. However, there are substantial gains in the run-time for vectorization across a matrix of $h,k$ values using Cox's methods.\n",
    "\n",
    "<center><h4>Figure 1: BVN comparison </h4></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <center>A: Orthant probabilities   </center> </td>\n",
    "        <td> <center>B: Vectorized run-times </center> </td>\n",
    "    <tr>\n",
    "        <td> <img src=\"figures/gg_pval.png\" style=\"width: 80%\"/> </td>\n",
    "        <td> <img src=\"figures/gg_rvec.png\" style=\"width: 90%\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "While this section might have seemed like something of a detour, its usefulness will made apparent soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Deriving $f_W$ and $f_W$\n",
    "\n",
    "The first step to characterize the distribution NTS, $W$, is to understand the distribution of a truncated bivariate normal distrubion (TBVN). \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X_1, X_2) &\\sim \\text{TBVN}(\\theta, \\Sigma, \\rho, a, b) \\\\\n",
    "E([X_1,X_2]) &= [\\theta_1, \\theta_2] \\\\\n",
    "V([X_1,X_2]) &= \\begin{pmatrix} \\sigma_1^2 & \\rho \\sigma_1\\sigma_2 \\\\ \\rho \\sigma_1\\sigma_2 & \\sigma_2^2  \\end{pmatrix} \\\\\n",
    "X_1 &\\in \\mathbb{R}, \\hspace{3mm} X_2 \\in [a,b]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notice that the TBVN takes the same formulation as a bivariate normal (a mean vector, a covariance matrix, and a correlation coefficient $\\rho$) except that the random variable $Y$ term is bound between $a$ and $b$. Arnold showed that the marginal density of the non-truncated random variable $X$ could be written as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_W &= f_{X_1}(x) = \\frac{\\phi(m_1(x)) \\Bigg[ \\Phi\\Big(\\frac{\\beta-\\rho \\cdot m_1(x)}{\\sqrt{1-\\rho^2}}\\Big) - \\Phi\\Big(\\frac{\\alpha-\\rho \\cdot m_1(x)}{\\sqrt{1-\\rho^2}}\\Big) \\Bigg] }{\\sigma_1\\cdot Z} \\tag{3} \\\\ \n",
    "m_1(x) &= (x - \\theta_1)/\\sigma_1  \\\\\n",
    "\\alpha &= \\frac{a-\\theta_2}{\\sigma_2}, \\hspace{3mm} \\beta = \\frac{b-\\theta_2}{\\sigma_2}  \\\\\n",
    "Z &= \\Phi(\\beta) - \\Phi(\\alpha)\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:arnold}\n",
    "\n",
    "Kim showed that a NTS could be written as a TBVN by a simple change of variables:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X_1 &= Z_1 + Z_2^u \\\\ \n",
    "X_2 &= Z_2^u, \\hspace{3mm} X_2 \\in [a,b] \\\\\n",
    "Z_2^u &\\sim N(\\mu_2,\\tau_2^2) \\\\\n",
    "\\theta &= [\\mu_1 + \\mu_2, \\mu_2] \\\\\n",
    "\\Sigma &= \\begin{pmatrix} \\tau_1^2 + \\tau_2^2 & \\rho \\sigma_1\\sigma_2 \\\\ \\rho \\sigma_1\\sigma_2 & \\tau_2^2  \\end{pmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $Z_2^u$ is the non-truncated version of $Z_2$. Clearly this specification of the TBVN is equivalent to the NTS and the marginal distribution of $X_1$ is equivalent to the PDF of $W$. Kim showed that the integral of $\\eqref{eq:arnold}$ was equivalent to solving the orthant probabilities or a standard BVN : \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_W &= F_{X_1}(x) = 1 - \\frac{L(m_1(x),\\alpha,\\rho) - L(m_1(x),\\beta,\\rho)}{Z} \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:kim}\n",
    "\n",
    "Hence, the PDF for a NTS can be solved analytically, and CDF can be solved using either numerical methods for the exact univariate integral in $\\eqref{eq:sheppard}$ or Cox's approximation method in $\\eqref{eq:cox}$. The code block below provides the python code necessary to calculate $\\eqref{eq:arnold}$ and $\\eqref{eq:kim}$. Although I have not implemented an efficient solution, the `NTS` class provides a quantile function $F^{-1}_W(p)=\\inf_w F_W(w) \\geq p$ that relies on [Golden's method](https://en.wikipedia.org/wiki/Golden-section_search) for univariate optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTS():\n",
    "    def __init__(self, mu, tau, a, b):\n",
    "        \"\"\"\n",
    "        mu: array of means\n",
    "        tau: array of standard errors\n",
    "        rho: correlation coefficient\n",
    "        \"\"\"\n",
    "        assert mu.shape[0]==tau.shape[0]==2\n",
    "        # Assign parameters\n",
    "        self.mu, self.tau = mu.flatten(), tau.flatten()\n",
    "        self.a, self.b = a, b\n",
    "        # Truncated normal (Z2)\n",
    "        self.alpha = (self.a - self.mu[1]) / self.tau[1]\n",
    "        self.beta = (self.b - self.mu[1]) / self.tau[1]\n",
    "        self.Z = norm.cdf(self.beta) - norm.cdf(self.alpha)\n",
    "        self.Q = norm.pdf(self.alpha) - norm.pdf(self.beta)\n",
    "        # Average will be unweighted combination of the two distributions\n",
    "        self.mu_W = self.mu[0] + self.mu[1] + self.tau[1]*self.Q/self.Z\n",
    "        # Distributions\n",
    "        self.dist_X1 = norm(loc=self.mu[0], scale=self.tau[0])\n",
    "        self.dist_X2 = truncnorm(a=self.alpha, b=self.beta, loc=self.mu[0], scale=self.tau[1])\n",
    "        # W\n",
    "        self.theta1 = self.mu.sum()\n",
    "        self.theta2 = self.mu[1]\n",
    "        self.sigma1 = np.sqrt(np.sum(self.tau**2))\n",
    "        self.sigma2 = self.tau[1]\n",
    "        self.rho = self.sigma2/self.sigma1\n",
    "\n",
    "    def pdf(self, x):\n",
    "        term1 = self.sigma1 * self.Z\n",
    "        m1 = (x - self.theta1) / self.sigma1\n",
    "        term2 = (self.beta-self.rho*m1)/np.sqrt(1-self.rho**2)\n",
    "        term3 = (self.alpha-self.rho*m1)/np.sqrt(1-self.rho**2)\n",
    "        f = norm.pdf(m1)*(norm.cdf(term2) - norm.cdf(term3)) / term1\n",
    "        return f\n",
    "\n",
    "    def cdf(self, x, method='scipy'):\n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x)\n",
    "        if isinstance(x, float) or isinstance(x, int):\n",
    "            x = np.array([x])\n",
    "        nx = len(x)\n",
    "        m1 = (x - self.theta1) / self.sigma1\n",
    "        bvn = BVN(mu=[0,0],sigma=[1,1],rho=self.rho)\n",
    "        orthant1 = bvn.orthant(m1,np.repeat(self.alpha,nx),method=method)\n",
    "        orthant2 = bvn.orthant(m1,np.repeat(self.beta,nx),method=method)\n",
    "        return 1 - (orthant1 - orthant2)/self.Z\n",
    "\n",
    "    def ppf(self, p):\n",
    "        if isinstance(p, list):\n",
    "            p = np.array(p)\n",
    "        if isinstance(p, float) or isinstance(p, int):\n",
    "            p = np.array([p])\n",
    "        # Set up reasonable lower bounds\n",
    "        lb = self.mu_W - self.sigma1*4\n",
    "        ub = self.mu_W + self.sigma1*4\n",
    "        w = np.repeat(np.NaN, len(p))\n",
    "        for i, px in enumerate(p):\n",
    "            tmp = float(minimize_scalar(fun=lambda w: (self.cdf(w)-px)**2,method='bounded',bounds=(lb,ub)).x)\n",
    "            w[i] = tmp\n",
    "        return w\n",
    "\n",
    "    def rvs(self, n, seed=1234):\n",
    "        r1 = self.dist_X1.rvs(size=n,random_state=seed)\n",
    "        r2 = self.dist_X2.rvs(size=n,random_state=seed)\n",
    "        return r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate NTS data with the following parameters: $\\mu_1=1$, $\\tau_1=1$ (for $Z_1$), $\\mu_2=1$, $\\tau_2=2$, $a=-1$, and $b=4$ (for $Z_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       method        mu\n",
      "0   Empirical  2.439796\n",
      "1      Theory  2.438245\n",
      "2  Quadrature  2.438245\n"
     ]
    }
   ],
   "source": [
    "# Demonstrated with example\n",
    "mu1, tau1 = 1, 1\n",
    "mu2, tau2, a, b = 1, 2, -1, 4\n",
    "mu, tau = np.array([mu1, mu2]), np.array([tau1,tau2])**2\n",
    "dist_NTS = NTS(mu=mu,tau=tau, a=a, b=b)\n",
    "n_iter = 100000\n",
    "W_sim = dist_NTS.rvs(n=n_iter,seed=1)\n",
    "mu_sim, mu_theory = W_sim.mean(),dist_NTS.mu_W\n",
    "xx = np.linspace(-5*mu.sum(),5*mu.sum(),n_iter)\n",
    "mu_quad = np.sum(xx*dist_NTS.pdf(xx)*(xx[1]-xx[0]))\n",
    "methods = ['Empirical','Theory', 'Quadrature']\n",
    "mus = [mu_sim, mu_theory, mu_quad]\n",
    "# Compare\n",
    "print(pd.DataFrame({'method':methods,'mu':mus}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above compares the empirical mean of the simulated data with the sum of the two location parameters of $Z_1$ and $Z_2$, as well as what we would estimate using a quadrature procedure with equation $\\eqref{eq:arnold}$, as $E(W)=\\int w F_W(w) dw$. Next, we can compare the actual and theoretical percentiles and quantiles against the empirical ones seen from `W_sim`. As expected, they are visually indistinguishable from each other.[[^2]]\n",
    "\n",
    "<center><h4>Figure 2: NTS P-P & Q-Q plot </h4></center>\n",
    "<img src=\"figures/gg_ppqq.png\" style=\"width: 70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.A) Application: Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.B) Two-stage testing\n",
    "\n",
    "In a [previous post](http://www.erikdrysdale.com/regression_trial/) I discussed a two-stage testing strategy designed to validate a machine learning regression model. The framework can be generalized as follows: 1) estimate an upper-bound on the mean of a Gaussian, 2) use this upper bound as the null hypothesis on a new sample of data. Note that a lower bound can be studied just as easily as an upper bond. We assume that $X, Z \\overset{iid}{\\sim} N(\\mu,\\sigma^2)$. Also, I'm going to assume that $n$ and $m$ are large enough so that the difference between the normal and student-t distribution are sufficiently small. The statistical pipeline is as follows.\n",
    "\n",
    "1. Estimate the mean and variance on the first Gaussian sample: $(X_{1},\\dots,X_{n}) \\sim N(\\bar{X},\\hat{\\sigma}_n^2)$\n",
    "2. Estimate the $1-\\gamma$ quantile of this distribution: $\\hat{\\mu}_0=\\bar{X}+\\hat{\\sigma}_n\\cdot\\Phi^{-1}_{1-\\gamma}$ to set the null hypothesis:\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_0&: \\mu \\geq \\hat{\\mu}_0 \\\\\n",
    "H_A&: \\mu < \\hat{\\mu}_0 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "3. Estimate mean and variance on second Gaussian sample: $(Z_{1},\\dots,Z_{m}) \\sim N(\\bar{Z},\\hat{\\sigma}_m^2)$\n",
    "4. Calculate a one-sided test statistic: $\\hat{s} = (\\bar{Z} - \\hat{\\mu}_0) / \\hat{\\sigma}_m$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Reject }H_0&: \\hat{s} < t_\\alpha\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Clearly $E(s)<0$ if $\\tau >0.5$. The statistical information provided by this procedure is two-fold. First, $\\mu_0$ has the property that it will be larger than the true mean only $\\tau$% of the time, so that we can say concomitantly say that $\\mu_0$ will bound the true mean $(1-\\tau)$% of the time (this is only in the frequentist sense, so it says nothing about any realization $\\hat{\\mu}_0$ itself). Second, when the null is false (i.e. the bound holds) we will be able to compute the power in advance (it will be a function of $m$).  \n",
    "\n",
    "Now, where does the NTS fit into this? In order to bound the type-I error rate to $\\alpha$, we need to know the distribution of $s$ when the null is true: $\\mu > \\hat{\\mu}_0$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{s} &= \\frac{\\bar{Z} - \\mu}{\\hat{\\sigma}_m} - \\frac{\\hat{\\mu}_0 - \\mu}{\\hat{\\sigma}_m} \\hspace{2mm} \\Big| \\hspace{2mm} \\hat{\\mu}_0 > \\mu  \\\\\n",
    "&= N(0,1) - \\frac{\\bar{X}-\\mu + \\hat{\\sigma}_n\\cdot\\Phi^{-1}_{1-\\gamma}}{\\hat{\\sigma}_m} \\hspace{2mm} \\Big| \\hspace{2mm} \\bar{X} + \\hat{\\sigma}_n\\cdot\\Phi^{-1}_{1-\\gamma} - \\mu > 0 \\\\\n",
    "&= N(0,1) + TN\\big( - \\sqrt{m/n} \\cdot \\Phi^{-1}_{1-\\gamma}, m/n, 0, \\infty  \\big)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence the critical value $t_\\alpha = F^{-1}_W(\\alpha)$ can be found by inverting $\\eqref{eq:kim}$ (i.e. the quantile function) with the following parameters from our original notation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu &= \\big[ 0, - \\sqrt{m/n} \\cdot \\Phi^{-1}_{1-\\gamma}\\big] \\\\\n",
    "\\tau^2 &= \\big[1, m/n\\big]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.C) Application: Data carving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "* * *\n",
    "\n",
    "### Footnotes\n",
    "\n",
    "[^1]: A standard BVN means that the means are zero, and covariance matrix has a value of one on the diagonals and $\\rho$ on the off-diagonals.\n",
    "\n",
    "[^2]: In reality, there are slight differences, they just cannot be seen on the figure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
