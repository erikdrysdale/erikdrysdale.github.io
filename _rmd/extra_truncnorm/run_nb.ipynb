{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Background\n",
    "\n",
    "Consider the sum of a normal $Z_1 \\sim N(\\mu_1,\\tau_1^2)$ and a truncated normal $Z_2 \\sim TN(\\mu_2,\\tau_2^2,a,b)$, denoted $W=Z_1+Z_2$. How can this normal-truncated sum (NTS) distribution be characterized? One approach for doing inference on $W$ would be to simply sample from $Z_1$ and $Z_2$, as can be done with most statistical software. However, having an analytical or numerical method for calculating the density (PDF) and cumulative distribution (CDF) of $W$ has several benefits over the sampling approach. This includes including vectorization, reproducibility across software programs, and faster compute times (especially for the tails of the distribution). This post will provide `python` code to calculate the PDF and CDF of an arbitrary NTS. The value of distribution will be highlighted with three use cases: i) quality control, ii) two-stage hypothesis testing, and iii) data carving for selective inference. \n",
    "\n",
    "This post will make extensive use of the theoretical results from [Arnold et. al (1993)](https://link.springer.com/article/10.1007/BF02294652) (hereafter Arnold) and [Kim (2006)](https://www.kss.or.kr/jounalDown.php?IDX=831) (hereafter Kim). Sections (2) and (3) are not original work, but instead show how to translate these research papers into usable code. In section (4), the two-stage hypothesis testing and data carving are original contributions in making the connection between these statistical problems and the NTS distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Integrating a bivariate normal CDF\n",
    "\n",
    "The CDF of a bivarate normal (BVN) will turn out to be necessary to calculate the CDF of a NTS. While there is no closed-form solution to the CDF of a BVN (unless the point of evaluation occurs at the origin), the integration problem can be quickly solved by numerical methods. Additionally, if the user is willing to accept a non-exact solution, there are approximation methods which yield an analytical solution that can be easily vectorized. This section will briefly review both approaches. \n",
    "\n",
    "Following the notation of the literature, the orthant probability of a standard BVN[[1]], $L(h,k,\\rho)=P(X_1\\geq h, X_2\\geq k)$, is equivalent to solving the following integral introduced by [Sheppard](https://royalsocietypublishing.org/doi/10.1098/rsta.1899.0003) 130 years ago:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(h,k,\\rho) &= \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_h^\\infty \\int_k^\\infty \\exp \\Bigg[ - \\frac{x^2-2\\rho x y + y^2}{2(1-\\rho^2)} \\Bigg] dx dy  \\\\\n",
    "&= \\frac{1}{2\\pi} \\int_{\\arccos(\\rho)}^{\\pi} \\exp \\Bigg[ - \\frac{h^2+k^2-2hk\\cos(\\theta)}{2\\sin^2(\\theta)} \\Bigg] d\\theta \\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:sheppard}\n",
    "\n",
    "Note that the orthant probability can easily be related to the CDF: $F(h,k,\\rho)=P(X_1\\leq h, X_2\\leq k) = L(h,k,\\rho) + \\Phi(h) + \\Phi(k) - 1$. The CDF and PDF of a standard normal are denoted by $\\Phi(\\cdot)$ and $\\phi(\\cdot)$, respectively. What is interesting about $\\eqref{eq:sheppard}$ is that the problem of integrating out both $X_1$ and $X_2$ can been reduced to a single variable of integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `python`, `scipy.integrate.quad` can be used to quickly solve $\\eqref{eq:sheppard}$. As will be shown, the method turns out to produce results  identical to that of the `scipy.stats.multivariate_normal.cdf` function. Even though there is no closed-form solution, [Cox 1991](https://www.jstor.org/stable/1403446) showed that $L(\\cdot)$ could be approximated using a simple Taylor expansion.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(h,k,\\rho) &= P(X_1 \\geq h, X_2 \\geq k) = P(X_1 \\geq h) P(X_2 \\geq k | X_1 \\geq h) \\\\\n",
    "&= \\Phi(-h) E\\Bigg[ \\Phi\\Bigg(\\frac{\\rho X_1 - k}{\\sqrt{1-\\rho^2}} \\Bigg) \\Bigg| X_1 \\geq h \\Bigg] \\\\\n",
    "&\\approx \\Phi(-h) \\Phi\\Bigg( \\frac{\\rho\\phi(h)/\\Phi(-h)-k}{\\sqrt{1-\\rho^2}} \\Bigg) \\tag{2}\n",
    "\\end{align} \n",
    "$$\n",
    "\\label{eq:cox}\n",
    "\n",
    "Cox showed that this approximation method works well for reasonable ranges of $\\rho$ (absolute coefficient less than 90%).  For users willing to trade off accuracy for speed, $\\eqref{eq:cox}$ allows for rapid vectorization cross different values of $\\rho$, $h$, or $k$. The code block below will define a `BVN` class that can be used to calculate the orthant probability and perform sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>orthant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scipy</td>\n",
       "      <td>0.040615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sheppard</td>\n",
       "      <td>0.040615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cox</td>\n",
       "      <td>0.040670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rvs</td>\n",
       "      <td>0.040866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     method   orthant\n",
       "0     scipy  0.040615\n",
       "1  sheppard  0.040615\n",
       "2       cox  0.040670\n",
       "3       rvs  0.040866"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, truncnorm, chi2, t\n",
    "from scipy.stats import multivariate_normal as MVN\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "\n",
    "\n",
    "class BVN():\n",
    "    def __init__(self, mu, sigma, rho):\n",
    "        \"\"\"\n",
    "        mu: array of means\n",
    "        sigma: array of variances\n",
    "        rho: correlation coefficient\n",
    "        \"\"\"\n",
    "        if isinstance(mu,list):\n",
    "            mu, sigma = np.array(mu), np.array(sigma)\n",
    "        assert mu.shape[0]==sigma.shape[0]==2\n",
    "        assert np.abs(rho) <= 1\n",
    "        self.mu = mu.reshape([1,2])\n",
    "        self.sigma = sigma.flatten()\n",
    "        od = rho*np.sqrt(sigma.prod())\n",
    "        self.rho = rho\n",
    "        self.Sigma = np.array([[sigma[0],od],[od, sigma[1]]])\n",
    "        self.A = cholesky(self.Sigma) # A.T.dot(A) = Sigma\n",
    "\n",
    "    def rvs(self, size, seed=None):\n",
    "        \"\"\"\n",
    "        size: number of samples to simulate\n",
    "        seed: to pass onto np.random.seed\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        X = np.random.randn(size,2)\n",
    "        Z = self.A.T.dot(X.T).T + self.mu\n",
    "        return Z\n",
    "\n",
    "    def sheppard(self, theta, h, k):\n",
    "        return (1/(2*np.pi))*np.exp(-0.5*(h**2+k**2-2*h*k*np.cos(theta))/(np.sin(theta)**2))\n",
    "\n",
    "    def orthant(self, h, k, method='scipy'):\n",
    "        assert method in ['scipy','cox','sheppard']\n",
    "        if isinstance(h,int) or isinstance(h, float):\n",
    "            h, k = np.array([h]), np.array([k])\n",
    "        else:\n",
    "            assert isinstance(h,np.ndarray) and isinstance(k,np.ndarray)\n",
    "        assert len(h) == len(k)\n",
    "        # Calculate the number of standard deviations away it is        \n",
    "        Y = (np.c_[h, k] - self.mu)/np.sqrt(self.sigma)\n",
    "        Y1, Y2 = Y[:,0], Y[:,1]\n",
    "\n",
    "        # (i) scipy: L(h, k)=1-(F1(h)+F2(k))+F12(h, k)\n",
    "        if method == 'scipy':\n",
    "            sp_bvn = MVN([0, 0],[[1,self.rho],[self.rho,1]])\n",
    "            pval = 1+sp_bvn.cdf(Y)-(norm.cdf(Y1)+norm.cdf(Y2))\n",
    "            return pval \n",
    "        \n",
    "        if method == 'cox':\n",
    "            mu_a = norm.pdf(Y1)/norm.cdf(-Y1)\n",
    "            root = np.sqrt(1-self.rho**2)\n",
    "            xi = (self.rho * mu_a - Y2) / root\n",
    "            pval = norm.cdf(-Y1) * norm.cdf(xi)\n",
    "            return pval\n",
    "\n",
    "        if method == 'sheppard':\n",
    "            pval = np.array([quad(self.sheppard, np.arccos(self.rho), np.pi, args=(y1,y2))[0] for y1, y2 in zip(Y1,Y2)])\n",
    "            return pval\n",
    "\n",
    "mu = np.array([1,2])\n",
    "sigma = np.array([0.5,2])\n",
    "dist_BVN = BVN(mu,sigma,rho=0.4)\n",
    "h, k = 2, 3\n",
    "pval_scipy = dist_BVN.orthant(h, k, method='scipy')[0]\n",
    "pval_sheppard = dist_BVN.orthant(h, k, method='sheppard')[0]\n",
    "pval_cox = dist_BVN.orthant(h, k, method='cox')[0]\n",
    "nsim = 1000000\n",
    "pval_rvs = dist_BVN.rvs(nsim)\n",
    "pval_rvs = pval_rvs[(pval_rvs[:,0]>h) & (pval_rvs[:,1]>k)].shape[0]/nsim\n",
    "methods = ['scipy','sheppard','cox','rvs']\n",
    "pvals = [pval_scipy, pval_sheppard, pval_cox, pval_rvs]\n",
    "pd.DataFrame({'method':methods,'orthant':pvals})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output results are identical using either the `multivariate_normal` class or directly calling integration solvers.  Figure 1A below shows that the Cox-method is very close to the numerical solution, although there are no definitive error bounds (in Cox's paper he found that the worst-case empirical error was around 10%). Although the figure is not shown, there is no improvement in the run time using the `sheppard` method over `scipy`'s built in MVN distribution for a single estimate, and `scipy` is faster over a matrix of values. There are substantial gains in the run time for vectorization across a matrix of $h,k$ values using Cox's methods (Figure 1B).\n",
    "\n",
    "<center><h4>Figure 1: BVN comparison </h4></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <center>A: Orthant probabilities   </center> </td>\n",
    "        <td> <center>B: Vectorized run-times </center> </td>\n",
    "    <tr>\n",
    "        <td> <img src=\"figures/gg_pval.png\" style=\"width: 80%\"/> </td>\n",
    "        <td> <img src=\"figures/gg_rvec.png\" style=\"width: 90%\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Deriving $f_W$ and $f_W$\n",
    "\n",
    "The first step to characterize the NTS distribution is to understand the distribution of a truncated bivariate normal distrubion (TBVN). \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X_1, X_2) &\\sim \\text{TBVN}(\\theta, \\Sigma, \\rho, a, b) \\\\\n",
    "E([X_1,X_2]) &= [\\theta_1, \\theta_2] \\\\\n",
    "V([X_1,X_2]) &= \\begin{pmatrix} \\sigma_1^2 & \\rho \\sigma_1\\sigma_2 \\\\ \\rho \\sigma_1\\sigma_2 & \\sigma_2^2  \\end{pmatrix} \\\\\n",
    "X_1 &\\in \\mathbb{R}, \\hspace{3mm} X_2 \\in [a,b]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notice that the TBVN takes the same formulation as a bivariate normal (a mean vector, a covariance matrix, and a correlation coefficient $\\rho$) except that the random variable $X_2$ term is bound between $a$ and $b$. Arnold showed that the marginal density of the non-truncated random variable $X_1$ could be written as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_W(x) &= f_{X_1}(x) = \\frac{\\phi(m_1(x)) \\Bigg[ \\Phi\\Big(\\frac{\\beta-\\rho \\cdot m_1(x)}{\\sqrt{1-\\rho^2}}\\Big) - \\Phi\\Big(\\frac{\\alpha-\\rho \\cdot m_1(x)}{\\sqrt{1-\\rho^2}}\\Big) \\Bigg] }{\\sigma_1\\cdot Z} \\tag{3} \\\\ \n",
    "m_1(x) &= (x - \\theta_1)/\\sigma_1  \\\\\n",
    "\\alpha &= \\frac{a-\\theta_2}{\\sigma_2}, \\hspace{3mm} \\beta = \\frac{b-\\theta_2}{\\sigma_2}  \\\\\n",
    "Z &= \\Phi(\\beta) - \\Phi(\\alpha)\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:arnold}\n",
    "\n",
    "Kim showed that a NTS could be written as a TBVN by a simple change of variables:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X_1 &= Z_1 + Z_2^u \\\\ \n",
    "X_2 &= Z_2^u, \\hspace{3mm} X_2 \\in [a,b] \\\\\n",
    "Z_2^u &\\sim N(\\mu_2,\\tau_2^2) \\\\\n",
    "\\theta &= [\\mu_1 + \\mu_2, \\mu_2] \\\\\n",
    "\\Sigma &= \\begin{pmatrix} \\tau_1^2 + \\tau_2^2 & \\rho \\sigma_1\\sigma_2 \\\\ \\rho \\sigma_1\\sigma_2 & \\tau_2^2  \\end{pmatrix} \\\\\n",
    "\\rho &= \\sigma_2 / \\sigma_1 = \\tau_2/\\sqrt{\\tau_1^2 + \\tau_2^2} \\\\\n",
    "Z_1 + Z_2 &= W \\sim NTS(\\theta(\\mu),\\Sigma(\\tau^2), a, b)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $Z_2^u$ is the non-truncated version of $Z_2$. Clearly this specification of the TBVN is equivalent to the NTS and the marginal distribution of $X_1$ is equivalent to the PDF of $W$. Kim showed that the integral of $\\eqref{eq:arnold}$ was equivalent to solving the orthant probabilities of a standard BVN : \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_W &= F_{X_1}(x) = 1 - \\frac{L(m_1(x),\\alpha,\\rho) - L(m_1(x),\\beta,\\rho)}{Z} \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:kim}\n",
    "\n",
    "Hence, the PDF for a NTS can be solved analytically, and CDF can be solved using either numerical methods for the exact univariate integral in $\\eqref{eq:sheppard}$ or Cox's approximation method in $\\eqref{eq:cox}$. The code block below provides the `python` code necessary to calculate $\\eqref{eq:arnold}$ and $\\eqref{eq:kim}$. The `NTS` class provides a quantile function $F^{-1}_W(p)=\\sup_w F_W(w) \\leq p$ that relies on [Golden's method](https://en.wikipedia.org/wiki/Golden-section_search) for univariate optimization by querying the `cdf` attribute until convergence (I am sure there are more efficient ways to carry out this optimization, especially for vectorization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTS():\n",
    "    def __init__(self, mu, tau, a, b):\n",
    "        \"\"\"\n",
    "        mu: array of means\n",
    "        tau: array of standard errors\n",
    "        rho: correlation coefficient\n",
    "        \"\"\"\n",
    "        assert mu.shape[0]==tau.shape[0]==2\n",
    "        # Assign parameters\n",
    "        self.mu, self.tau = mu.flatten(), tau.flatten()\n",
    "        self.a, self.b = a, b\n",
    "        # Truncated normal (Z2)\n",
    "        self.alpha = (self.a - self.mu[1]) / self.tau[1]\n",
    "        self.beta = (self.b - self.mu[1]) / self.tau[1]\n",
    "        self.Z = norm.cdf(self.beta) - norm.cdf(self.alpha)\n",
    "        self.Q = norm.pdf(self.alpha) - norm.pdf(self.beta)\n",
    "        # Average will be unweighted combination of the two distributions\n",
    "        self.mu_W = self.mu[0] + self.mu[1] + self.tau[1]*self.Q/self.Z\n",
    "        # Distributions\n",
    "        self.dist_X1 = norm(loc=self.mu[0], scale=self.tau[0])\n",
    "        self.dist_X2 = truncnorm(a=self.alpha, b=self.beta, loc=self.mu[0], scale=self.tau[1])\n",
    "        # W\n",
    "        self.theta1 = self.mu.sum()\n",
    "        self.theta2 = self.mu[1]\n",
    "        self.sigma1 = np.sqrt(np.sum(self.tau**2))\n",
    "        self.sigma2 = self.tau[1]\n",
    "        self.rho = self.sigma2/self.sigma1\n",
    "\n",
    "    def pdf(self, x):\n",
    "        term1 = self.sigma1 * self.Z\n",
    "        m1 = (x - self.theta1) / self.sigma1\n",
    "        term2 = (self.beta-self.rho*m1)/np.sqrt(1-self.rho**2)\n",
    "        term3 = (self.alpha-self.rho*m1)/np.sqrt(1-self.rho**2)\n",
    "        f = norm.pdf(m1)*(norm.cdf(term2) - norm.cdf(term3)) / term1\n",
    "        return f\n",
    "\n",
    "    def cdf(self, x, method='scipy'):\n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x)\n",
    "        if isinstance(x, float) or isinstance(x, int):\n",
    "            x = np.array([x])\n",
    "        nx = len(x)\n",
    "        m1 = (x - self.theta1) / self.sigma1\n",
    "        bvn = BVN(mu=[0,0],sigma=[1,1],rho=self.rho)\n",
    "        orthant1 = bvn.orthant(m1,np.repeat(self.alpha,nx),method=method)\n",
    "        orthant2 = bvn.orthant(m1,np.repeat(self.beta,nx),method=method)\n",
    "        return 1 - (orthant1 - orthant2)/self.Z\n",
    "\n",
    "    def ppf(self, p):\n",
    "        if isinstance(p, list):\n",
    "            p = np.array(p)\n",
    "        if isinstance(p, float) or isinstance(p, int):\n",
    "            p = np.array([p])\n",
    "        # Set up reasonable lower bounds\n",
    "        lb = self.mu_W - self.sigma1*4\n",
    "        ub = self.mu_W + self.sigma1*4\n",
    "        w = np.repeat(np.NaN, len(p))\n",
    "        for i, px in enumerate(p):\n",
    "            tmp = float(minimize_scalar(fun=lambda w: (self.cdf(w)-px)**2,method='bounded',bounds=(lb,ub)).x)\n",
    "            w[i] = tmp\n",
    "        return w\n",
    "\n",
    "    def rvs(self, n, seed=1234):\n",
    "        r1 = self.dist_X1.rvs(size=n,random_state=seed)\n",
    "        r2 = self.dist_X2.rvs(size=n,random_state=seed)\n",
    "        return r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate NTS data with the following parameters: $\\mu_1=1$, $\\tau_1=1$ (for $Z_1$), $\\mu_2=1$, $\\tau_2=2$, $a=-1$, and $b=4$ (for $Z_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       method        mu\n",
      "0   Empirical  2.292354\n",
      "1      Theory  2.290375\n",
      "2  Quadrature  2.290375\n"
     ]
    }
   ],
   "source": [
    "# Demonstrated with example\n",
    "mu1, tau1 = 1, 1\n",
    "mu2, tau2, a, b = 1, 2, -1, 4\n",
    "mu, tau = np.array([mu1, mu2]), np.array([tau1,tau2])\n",
    "dist_NTS = NTS(mu=mu,tau=tau, a=a, b=b)\n",
    "n_iter = 100000\n",
    "W_sim = dist_NTS.rvs(n=n_iter,seed=1)\n",
    "mu_sim, mu_theory = W_sim.mean(),dist_NTS.mu_W\n",
    "xx = np.linspace(-5*mu.sum(),5*mu.sum(),n_iter)\n",
    "mu_quad = np.sum(xx*dist_NTS.pdf(xx)*(xx[1]-xx[0]))\n",
    "methods = ['Empirical','Theory', 'Quadrature']\n",
    "mus = [mu_sim, mu_theory, mu_quad]\n",
    "# Compare\n",
    "print(pd.DataFrame({'method':methods,'mu':mus}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above compares the empirical mean of the simulated data with the sum of the two location parameters of $Z_1$ and $Z_2$, as well as what we would estimate using a quadrature procedure with equation $\\eqref{eq:arnold}$, as $E(W)=\\int w F_W(w) dw \\approx \\sum w F_W(w) dw$ for a small $dw$. Next, we can compare the actual and theoretical percentiles and quantiles against the empirical ones seen from `W_sim`. As expected, they are visually indistinguishable from each other.[[^2]]\n",
    "\n",
    "<center><h4>Figure 2: NTS P-P & Q-Q plot </h4></center>\n",
    "<img src=\"figures/gg_ppqq.png\" style=\"width: 60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.A) Application: Quality control\n",
    "\n",
    "In some manufacturing processes, the one of the components may go through an quality control procedure that removes items above or below a certain threshold. For example, this question was posed in a [1964 issue](https://www.jstor.org/stable/1266101?seq=1) of *Technometrics*:\n",
    "\n",
    "> An item which we make has, among others, two parts which are assembled additively with regard to length. The lengths of both parts are normally distributed but, before assembly, one of the parts is subjected to an inspection which removes all individuals below a specified length. As an example, suppose that X comes from a normal distribution with a mean 100 and a standard deviation of 6, and Y comes from a normal distribution with a mean of 50 and a standard deviation of 3, but with the restriction that Y > 44. How can I find the chance that X + Y is equal to or less than a given value?\n",
    "\n",
    "Subsequent answers focused on value of $P(X + Y < 138) \\approx 0.03276$. We can confirm this by using the `NTS` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X+Y<138)=0.03276\n"
     ]
    }
   ],
   "source": [
    "mu1, tau1 = 100, 6\n",
    "mu2, tau2, a, b = 50, 3, 44, np.inf\n",
    "mu, tau = np.array([mu1, mu2]), np.array([tau1,tau2])\n",
    "dist_A = NTS(mu=mu,tau=tau, a=a, b=b)\n",
    "print('P(X+Y<138)=%0.5f' % dist_A.cdf(138))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.B) Two-stage testing\n",
    "\n",
    "In a [previous post](http://www.erikdrysdale.com/regression_trial/) I discussed a two-stage testing strategy designed to validate a machine learning regression model. The framework can be generalized as follows: 1) estimate an upper-bound on the mean of a Gaussian, 2) use this upper bound as the null hypothesis on a new sample of data.[[^3]] This is useful in order to reject the null (in the second stage) to say the mean is *at most* some value. Several simplifying assumptions are made to make the analysis tractable: the data are [IID](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) and from the same normal distribution: $S, T \\overset{iid}{\\sim} N(\\delta,\\sigma^2)$, and that $n$ and $m$ are large enough so that the difference between the normal and student-t distributions are sufficiently small. The statistical pipeline is as follows.\n",
    "\n",
    "1. Estimate the mean and variance on the first Gaussian sample: $(S_{1},\\dots,S_{n}) \\sim N(\\hat{\\delta}_S,\\hat{\\sigma}^2/n)$\n",
    "2. Estimate the $1-\\gamma$ quantile of this distribution: $\\hat{\\delta}_0=\\hat{\\delta}_S+n^{-1/2}\\cdot\\hat{\\sigma}\\cdot\\Phi^{-1}_{1-\\gamma}$ to set the null hypothesis:\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_0&: \\delta \\geq \\hat{\\delta}_0 \\\\\n",
    "H_A&: \\delta < \\hat{\\delta}_0 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "3. Estimate mean and variance on second Gaussian sample: $(T_{1},\\dots,T_{m}) \\sim N(\\hat{\\delta}_T,\\hat{\\sigma}^2)$\n",
    "4. Calculate a one-sided test statistic: $\\hat{s} = \\sqrt{m} \\cdot (\\hat{\\delta}_T - \\hat{\\delta}_0) / \\hat{\\sigma}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Reject }H_0&: \\hat{s} < t_\\alpha\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that when the null is true, then the estimate of the upper bound is too low because $\\delta$ is larger than what we estimate ($\\hat{\\delta}_0$). Concomitantly, the null being false implies that we have successfully bounded the actual mean. If $\\tau >0.5$, then $E(s)<0$ because $\\hat{\\delta}_0$ will tend to be above the true average of $\\delta$. The statistical information provided by this procedure is two-fold. First, $\\delta_0$ has the property that it will be larger than the true mean $(1-\\tau)$% of the time.[[^4]] Second, when the null is false (i.e. the bound holds) we will be able to compute the power in advance.  Now, where does the NTS fit into this? In order to bound the type-I error rate to $\\alpha$, we need to know the distribution of $s$ when the null is true: $\\delta > \\hat{\\delta}_0$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{s} &= \\frac{\\hat{\\delta}_T - \\delta}{\\hat{\\sigma}_m} - \\frac{\\hat{\\delta}_0 - \\delta}{\\hat{\\sigma}_m} \\hspace{2mm} \\Big| \\hspace{2mm} \\hat{\\delta}_0 > \\delta  \\\\\n",
    "&= N(0,1) - \\frac{\\hat{\\delta}_S-\\delta + \\hat{\\sigma}_n\\cdot\\Phi^{-1}_{1-\\gamma}}{\\hat{\\sigma}_m} \\hspace{2mm} \\Big| \\hspace{2mm} \\hat{\\delta}_S + \\hat{\\sigma}_n\\cdot\\Phi^{-1}_{1-\\gamma} - \\delta > 0 \\\\\n",
    "&= N(0,1) + TN\\big( - \\sqrt{m/n} \\cdot \\Phi^{-1}_{1-\\gamma}, m/n, 0, \\infty  \\big)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence the critical value $t_\\alpha = F^{-1}_W(\\alpha)$ can be found by inverting $\\eqref{eq:kim}$ (i.e. the quantile function) with the following parameters from our original notation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu &= \\big[ 0, - \\sqrt{m/n} \\cdot \\Phi^{-1}_{1-\\gamma}\\big] \\\\\n",
    "\\tau^2 &= \\big[1, m/n \\big] \\\\\n",
    "\\hat{s} | H_0 &\\sim NTS(\\theta(\\mu), \\Sigma(\\tau^2), 0, \\infty ) \\\\ \n",
    "\\hat{s} | H_A &\\sim NTS(\\theta(\\mu), \\Sigma(\\tau^2), -\\infty, 0 )\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the distribution of the NTS test statistic only depends on $m$, $n$, and $\\gamma$. This means that the test statistic $s$ is a [pivot](https://en.wikipedia.org/wiki/Pivotal_quantity) over all possible values of $\\delta$ or $\\sigma$ (which are [nuisance parameters](https://en.wikipedia.org/wiki/Nuisance_parameter)). This means that the researcher can calculate the critical value $t_\\alpha$ as well as estimate the power $1-\\beta = P(\\hat{s}<t_\\alpha)$ in advance of the data. To the extent that their are degrees of freedom in selecting $m$, $n$, and $\\gamma$, several trade-offs occur.\n",
    "\n",
    "1. Smaller values of $\\gamma$ increase power increase but lower statistical information with a higher average upper bound\n",
    "2. Higher values of $n$ (keeping $m$ and $\\gamma$ fixed) reduce power but increase statistical information with lower average upper bound\n",
    "3. Higher values of $m$ (keeping $n and $\\gamma$ fixed) increase statistical power\n",
    "\n",
    "If $m+n=k$ is fixed, then a trade-off can be made between the size of the upper bound from the first stage, and the power in the second stage. The simulations check that the $\\hat{s}$ is characterized by a NTS distribution, and examine how the power and statistical information of the test varies for the following parameters: $\\delta=2$, $\\sigma^2=4$, $n+m=200$, $\\gamma=0.01$, and $\\alpha=0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_stage():\n",
    "    def __init__(self, n, m, gamma, alpha, pool=True, student=True):\n",
    "        # Assign\n",
    "        assert (n > 1) and (m >= 1) and (gamma > 0) & (gamma < 1)\n",
    "        self.n, self.m, self.gamma = n, m, gamma\n",
    "        self.pool = alpha, pool\n",
    "\n",
    "        # Calculate degres of freedom\n",
    "        self.dof_S, self.dof_T = n - 1, m - 1\n",
    "        if self.pool:\n",
    "            self.dof_T = n + m - 1\n",
    "        if student:\n",
    "            self.phi_inv = t(df=self.dof_S).ppf(1-gamma)\n",
    "        else:\n",
    "            self.phi_inv = norm.ppf(1-gamma)\n",
    "        mn_ratio = (self.dof_T+1)/(self.dof_S+1)\n",
    "        mu_2stage = np.array([0, -np.sqrt(mn_ratio)*self.phi_inv])\n",
    "        tau_2stage = np.sqrt([1, mn_ratio])\n",
    "        self.H0 = NTS(mu=mu_2stage,tau=tau_2stage, a=0, b=np.infty)\n",
    "        self.HA = NTS(mu=mu_2stage,tau=tau_2stage, a=-np.infty, b=0)\n",
    "        self.t_alpha = self.H0.ppf(alpha)[0]\n",
    "        self.power = self.HA.cdf(self.t_alpha)\n",
    "\n",
    "    def rvs(self, nsim, delta, sigma2, seed=None):\n",
    "        if seed is None:\n",
    "            seed = nsim\n",
    "        np.random.seed(seed)\n",
    "        delta1 = delta + np.sqrt(sigma2/(self.dof_S+1))*np.random.randn(nsim)\n",
    "        delta2 = delta + np.sqrt(sigma2/(self.dof_T+1))*np.random.randn(nsim)\n",
    "        sigS = np.sqrt(sigma2*chi2(df=self.dof_S).rvs(nsim)/self.dof_S)\n",
    "        sigT = np.sqrt(sigma2*chi2(df=self.dof_T).rvs(nsim)/self.dof_T)\n",
    "        delta0 = delta1 + (sigS/np.sqrt(self.n))*self.phi_inv\n",
    "        shat = (delta2 - delta0)/(sigT/np.sqrt(self.dof_T+1))\n",
    "        df = pd.DataFrame({'shat':shat, 'd0hat':delta0})\n",
    "        return df\n",
    "\n",
    "delta, sigma2 = 2, 4\n",
    "n, m = 100, 100\n",
    "gamma, alpha = 0.01, 0.05\n",
    "nsim = 10000000\n",
    "p_seq = np.arange(0.01,1,0.01)\n",
    "two_stage(n=n, m=m, gamma=gamma, alpha=alpha, pool=True).power[0]\n",
    "\n",
    "# --- (A) CALCULATE N=M=100 PP-QQ PLOT --- #\n",
    "dist_2s = two_stage(n=n, m=m, gamma=gamma, alpha=alpha, pool=True)\n",
    "df_2s = dist_2s.rvs(nsim=nsim, delta=delta, sigma2=sigma2)\n",
    "df_2s = df_2s.assign(Null=lambda x: x.d0hat < delta)\n",
    "df_2s = df_2s.assign(reject=lambda x: x.shat < dist_2s.t_alpha)\n",
    "\n",
    "qq_emp = df_2s.groupby('Null').apply(lambda x: pd.DataFrame({'pp':p_seq,'qq':np.quantile(x.shat,p_seq)}))\n",
    "qq_emp = qq_emp.reset_index().drop(columns='level_1')\n",
    "qq_theory_H0 = dist_2s.H0.ppf(p_seq)\n",
    "qq_theory_HA = dist_2s.HA.ppf(p_seq)\n",
    "tmp1 = pd.DataFrame({'pp':p_seq,'theory':qq_theory_H0,'Null':True})\n",
    "tmp2 = pd.DataFrame({'pp':p_seq,'theory':qq_theory_HA,'Null':False})\n",
    "qq_pp = qq_emp.merge(pd.concat([tmp1, tmp2]))\n",
    "qq_pp = qq_pp.melt(['pp','Null'],['qq','theory'],'tt')\n",
    "\n",
    "# --- (B) POWER AS GAMMA VARIES --- #\n",
    "gamma_seq = np.round(np.arange(0.01,0.21,0.01),2)\n",
    "power_theory = np.array([two_stage(n=n, m=m, gamma=g, alpha=alpha, pool=False).power[0] for g in gamma_seq])\n",
    "ub_theory = delta + np.sqrt(sigma2/n)*t(df=n-1).ppf(1-gamma_seq)\n",
    "power_emp, ub_emp = np.zeros(power_theory.shape), np.zeros(ub_theory.shape)\n",
    "for i, g in enumerate(gamma_seq):\n",
    "    tmp_dist = two_stage(n=n, m=m, gamma=g, alpha=alpha, pool=False)\n",
    "    tmp_sim = tmp_dist.rvs(nsim=nsim, delta=delta, sigma2=sigma2)\n",
    "    tmp_sim = tmp_sim.assign(Null=lambda x: x.d0hat < delta, \n",
    "                reject=lambda x: x.shat < tmp_dist.t_alpha)\n",
    "    power_emp[i] = tmp_sim.query('Null==False').reject.mean()\n",
    "    ub_emp[i] = tmp_sim.d0hat.mean()\n",
    "\n",
    "tmp1 = pd.DataFrame({'tt':'theory','gamma':gamma_seq,'power':power_theory,'ub':ub_theory})\n",
    "tmp2 = pd.DataFrame({'tt':'emp','gamma':gamma_seq,'power':power_emp,'ub':ub_emp})\n",
    "dat_gamma = pd.concat([tmp1, tmp2]).melt(['tt','gamma'],None,'msr')\n",
    "\n",
    "# --- (C) POWER AS N = K - M VARIES --- #\n",
    "\n",
    "k = n + m\n",
    "n_seq = np.arange(5,k,5)\n",
    "dat_nm = pd.concat([pd.DataFrame({'n':nn,'m':k-nn,\n",
    "    'power':two_stage(n=nn, m=k-nn, gamma=gamma, alpha=alpha, pool=True).power[0]},index=[nn]) for nn in n_seq])\n",
    "dat_nm = dat_nm.reset_index(None,True).assign(ub=delta + np.sqrt(sigma2/n_seq)*t(df=n_seq-1).ppf(1-gamma))\n",
    "dat_nm = dat_nm.melt(['n','m'],None,'msr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h4>Figure 3: Two-stage test statistic </h4></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <center>A: PP-QQ plot   </center> </td>\n",
    "        <td> <center>B: $\\gamma$ variation </center> </td>\n",
    "        <td> <center>C: $n,m$ variation </center> </td>\n",
    "    <tr>\n",
    "        <td> <img src=\"figures/gg_qp_2s.png\" style=\"width: 80%\"/> </td>\n",
    "        <td> <img src=\"figures/gg_gamma.png\" style=\"width: 80%\"/> </td>\n",
    "        <td> <img src=\"figures/gg_nm.png\" style=\"width: 80%\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The panel of plots in Figure 3 shows the trade-offs inherent in the choice of $m$, $n$, and $\\gamma$. Because the actual and expected quantiles line up perfectly (Figure 3A), we can be confident that the NTS distribution correctly describes the two-stage test statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.C) Application: Data carving\n",
    "\n",
    "The classical instruments of statistical inference like p-values and confidence internals have probabilistic properties that rely on the assumption that the choice of model and null hypotheses are specified in advance of the data. In other words, which variables will be tested and the form of the null hypothesis is determined independent of any data. In the age of exploratory statistics and large datasets these assumptions are increasingly at odds with empirical practice. For example, researchers who use a [lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) model to find a sparse set of coefficients will often \"investigate\" the importance of the features by examining the frequency of which variables get selected by the procedure through bootstrapping or cross-validation.[[^5]] Unfortunately the traditional methods of doing inference on the coefficients of a regression model do not work for the lasso because which coefficients get selected is a function of the data. In other words, both the model and the null hypothesis is determined *ex post* rather than *a priori*. \n",
    "\n",
    "In the framework of Fithian, Sun, and Taylor (see [*Optimal Inference After Model Selection*](https://arxiv.org/abs/1410.2597)), this amount to a two-stage process: \"1) the **selection stage** determines what questions to ask, and 2) the **inference stage** answers those questions.\" Once again echoing Fithian et al., this is moving from the non-adaptive to the adaptive selection paradigm (a.k.a. data snooping). There paper answer the following question: \"what it means for inference to be valid in the presence of adaptive selection and to propose methods that achieve this selective validity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "* * *\n",
    "\n",
    "### Footnotes\n",
    "\n",
    "[^1]: A standard BVN means that the means are zero, and covariance matrix has a value of one on the diagonals and $\\rho$ on the off-diagonals.\n",
    "\n",
    "[^2]: In reality, there are slight differences, they just cannot be seen on the figure.\n",
    "\n",
    "[^3]: A lower bound can be studied just as easily as an upper bond.\n",
    "\n",
    "[^4]: This is only true in the frequentist sense. We can say nothing about any one realization of $\\hat{\\delta}_0$ itself, only about the random variable $\\delta_0$ in general.\n",
    "\n",
    "[^5]: Here are just three examples of papers that use this frequency property found from the first page of a google search: [here](https://www.scirp.org/html/4-1240172_30157.htm), [here](https://dl.acm.org/doi/10.1145/1553374.1553431), and [here](https://www.frontiersin.org/articles/10.3389/fnagi.2016.00318/full)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
