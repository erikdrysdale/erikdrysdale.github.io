{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Background\n",
    "\n",
    "Consider the sum of a normal $Z_1 \\sim N(\\mu_1,\\tau_1^2)$ and a truncated normal $Z_2 \\sim TN(\\mu_2,\\tau_2^2,a,b)$, $W=Z_1+Z_2$. How can this normal-truncated sum (NTS) distribution be characterized? One approach for doing inference on $W$ is simply to sample from it, as sampling algorithms for both $Z_1$ and $Z_2$ are found in most statistical software. However, having an analytical or numerical method for calculating the density (PDF) and cumulative distribution (CDF) of $W$ has several benefits over the sampling approach. This includes including vectorization, reproducibility across software programs, and faster compute times (especially for the tails of the distribution). This post will provide `python` code to calculate the PDF and CDF of an arbitrary NTS. The value of distribution will be highlighted with three use cases: i) quality control, ii) two-stage hypothesis testing, and iii) data carving for selective inference. \n",
    "\n",
    "This post will make extensive use of the theoretical results from [Arnold et. al (1993)](https://link.springer.com/article/10.1007/BF02294652) (hereafter Arnold) and [Kim (2006)](https://www.kss.or.kr/jounalDown.php?IDX=831) (hereafter Kim). Sections (2) and (3) are not original work, but instead show how to translate these research papers into usable code. The two-stage hypothesis testing and data carving are unique in making the connection between these statistical problems and the NTS distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Integrating a bivariate normal CDF\n",
    "\n",
    "The CDF of a bivarate normal (BVN) will turn out to be necessary to calculate the CDF of a NTS. While there is no closed-form solution to the CDF of a BVN (unless it's from the origin), the integration problem can be quickly solved by numerical methods. Additionally, if the user is willing to accept a non-exact solution, there are approximation methods which yield an analytical solution that can be easily vectorized. This section will briefly review both approaches. \n",
    "\n",
    "Following the notation of the literature, the orthant probability of a standard BVN[[1]] $L(h,k,\\rho)=P(X_1\\geq h, X_2\\geq k)$ is equivalent to solving the following integral introduced by [Sheppard](https://royalsocietypublishing.org/doi/10.1098/rsta.1899.0003) 130 years ago:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(h,k,\\rho) &= \\frac{1}{2\\pi\\sqrt{1-\\rho^2}} \\int_h^\\infty \\int_k^\\infty \\exp \\Bigg[ - \\frac{x^2-2\\rho x y + y^2}{2(1-\\rho^2)} \\Bigg] dx dy  \\\\\n",
    "&= \\frac{1}{2\\pi} \\int_{\\arccos(\\rho)}^{\\pi} \\exp \\Bigg[ - \\frac{h^2+k^2-2hk\\cos(\\theta)}{2\\sin^2(\\theta)} \\Bigg] d\\theta \\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:sheppard}\n",
    "\n",
    "Note that the orthant probability can easily be related to the CDF: $F(h,k,\\rho)=P(X_1\\leq h, X_2\\leq k) = L(h,k,\\rho) + \\Phi(h) + \\Phi(k) - 1$. The CDF and PDF of a standard normal are denoted by $\\Phi(\\cdot)$ and $\\phi(\\cdot)$, respectively. What is interesting about $\\eqref{eq:sheppard}$ is that the problem of integrating out both $X_1$ and $X_2$ can been reduced to a single variable of integration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `python`, the `scipy.integrate.quad` can be used to quickly solve $\\eqref{eq:sheppard}$ quickly. As will be shown, the method turns out to produce results  identical to that of the `scipy.stats.multivariate_normal.cdf` function. Even though there is no closed-form solution, [Cox 1991](https://www.jstor.org/stable/1403446) showed that the estimate of $L(\\cdot)$ could be approximated using a simple Taylor expansion.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(h,k,\\rho) &= P(X_1 \\geq h, X_2 \\geq k) = P(X_1 \\geq h) P(X_2 \\geq k | X_1 \\geq h) \\\\\n",
    "&= \\Phi(-h) E\\Bigg[ \\Phi\\Bigg(\\frac{\\rho X_1 - k}{\\sqrt{1-\\rho^2}} \\Bigg) \\Bigg| X_1 \\geq h \\Bigg] \\\\\n",
    "&\\approx \\Phi(-h) \\Phi\\Bigg( \\frac{\\rho\\phi(h)/\\Phi(-h)-k}{\\sqrt{1-\\rho^2}} \\Bigg) \\tag{2}\n",
    "\\end{align} \n",
    "$$\n",
    "\\label{eq:cox}\n",
    "\n",
    "Cox showed that this approximation method works well for reasonable ranges of $\\rho$ (absolute coefficient less than 90%).  For users willing to trade off accuracy for speed, $\\eqref{eq:cox}$ allows for rapid vectorization cross different values of $\\rho$, $h$, or $k$. The code block below will define a `BVN` class that can be used to calculate the orthant probability and perform sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>orthant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scipy</td>\n",
       "      <td>0.040615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sheppard</td>\n",
       "      <td>0.040615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cox</td>\n",
       "      <td>0.040670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rvs</td>\n",
       "      <td>0.040819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     method   orthant\n",
       "0     scipy  0.040615\n",
       "1  sheppard  0.040615\n",
       "2       cox  0.040670\n",
       "3       rvs  0.040819"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, truncnorm\n",
    "from scipy.stats import multivariate_normal as MVN\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "class BVN():\n",
    "    # mu=np.array([1,2]);sigma=np.array([2,3]); rho=0.5; # del mu, sigma, rho, seed\n",
    "    def __init__(self, mu, sigma, rho):\n",
    "        \"\"\"\n",
    "        mu: array of means\n",
    "        sigma: array of variances\n",
    "        rho: correlation coefficient\n",
    "        \"\"\"\n",
    "        if isinstance(mu,list):\n",
    "            mu, sigma = np.array(mu), np.array(sigma)\n",
    "        assert mu.shape[0]==sigma.shape[0]==2\n",
    "        assert np.abs(rho) <= 1\n",
    "        self.mu = mu.reshape([1,2])\n",
    "        self.sigma = sigma.flatten()\n",
    "        od = rho*np.sqrt(sigma.prod())\n",
    "        self.rho = rho\n",
    "        self.Sigma = np.array([[sigma[0],od],[od, sigma[1]]])\n",
    "        self.A = cholesky(self.Sigma) # A.T.dot(A) = Sigma\n",
    "\n",
    "    # size=1000;seed=1234 # del size, seed\n",
    "    def rvs(self, size, seed=None):\n",
    "        \"\"\"\n",
    "        size: number of samples to simulate\n",
    "        seed: to pass onto np.random.seed\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        X = np.random.randn(size,2)\n",
    "        Z = self.A.T.dot(X.T).T + self.mu\n",
    "        return Z\n",
    "\n",
    "    def imills(self, a):\n",
    "        return norm.pdf(a)/norm.cdf(-a)\n",
    "\n",
    "    def sheppard(self, theta, h, k):\n",
    "        return (1/(2*np.pi))*np.exp(-0.5*(h**2+k**2-2*h*k*np.cos(theta))/(np.sin(theta)**2))\n",
    "\n",
    "    # self=dist_BVN; h, k = np.array([1,2,3]), np.array([2,3,4])\n",
    "    def orthant(self, h, k, method='scipy'):\n",
    "        # P(X1 >= h, X2 >=k)\n",
    "        assert method in ['scipy','cox','sheppard']\n",
    "        if isinstance(h,int) or isinstance(h, float):\n",
    "            h, k = np.array([h]), np.array([k])\n",
    "        else:\n",
    "            assert isinstance(h,np.ndarray) and isinstance(k,np.ndarray)\n",
    "        assert len(h) == len(k)\n",
    "        # assert np.all(h >= 0) and np.all(k >= 0)\n",
    "        # Calculate the number of standard deviations away it is        \n",
    "        Y = (np.c_[h, k] - self.mu)/np.sqrt(self.sigma)\n",
    "        Y1, Y2 = Y[:,0], Y[:,1]\n",
    "\n",
    "        # (i) scipy: L(h, k)=1-(F1(h)+F2(k))+F12(h, k)\n",
    "        if method == 'scipy':\n",
    "            sp_bvn = MVN([0, 0],[[1,self.rho],[self.rho,1]])\n",
    "            pval = 1+sp_bvn.cdf(Y)-(norm.cdf(Y1)+norm.cdf(Y2))\n",
    "            return pval \n",
    "        \n",
    "        # A Simple Approximation for Bivariate and Trivariate Normal Integrals\n",
    "        if method == 'cox':\n",
    "            mu_a = self.imills(Y1)\n",
    "            root = np.sqrt(1-self.rho**2)\n",
    "            xi = (self.rho * mu_a - Y2) / root\n",
    "            pval = norm.cdf(-Y1) * norm.cdf(xi)\n",
    "            return pval\n",
    "\n",
    "        if method == 'sheppard':\n",
    "            pval = np.array([quad(self.sheppard, np.arccos(self.rho), np.pi, args=(y1,y2))[0] for y1, y2 in zip(Y1,Y2)])\n",
    "            return pval\n",
    "\n",
    "mu = np.array([1,2])\n",
    "sigma = np.array([0.5,2])\n",
    "dist_BVN = BVN(mu,sigma,rho=0.4)\n",
    "h, k = 2, 3\n",
    "pval_scipy = dist_BVN.orthant(h, k, method='scipy')[0]\n",
    "pval_sheppard = dist_BVN.orthant(h, k, method='sheppard')[0]\n",
    "pval_cox = dist_BVN.orthant(h, k, method='cox')[0]\n",
    "nsim = 1000000\n",
    "pval_rvs = dist_BVN.rvs(nsim)\n",
    "pval_rvs = pval_rvs[(pval_rvs[:,0]>h) & (pval_rvs[:,1]>k)].shape[0]/nsim\n",
    "methods = ['scipy','sheppard','cox','rvs']\n",
    "pvals = [pval_scipy, pval_sheppard, pval_cox, pval_rvs]\n",
    "pd.DataFrame({'method':methods,'orthant':pvals})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output results are identical using either the `multivariate_normal` class or directly calling integration solvers.  Figure 1A below shows that the Cox-method is very close to the numerical solution, although there are no definitive error bounds (in Cox's paper he found that the worst-case empirical error was around 10%). Although the figure is not shown, there is no improvement in the run time using the `sheppard` method over `scipy`'s built in MVN distribution for a single estimate (and `scipy` does better for a vector/matrix of value. However, there are substantial gains in the run time for vectorization across a matrix of $h,k$ values using Cox's methods (Figure 1B).\n",
    "\n",
    "<center><h4>Figure 1: BVN comparison </h4></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <center>A: Orthant probabilities   </center> </td>\n",
    "        <td> <center>B: Vectorized run-times </center> </td>\n",
    "    <tr>\n",
    "        <td> <img src=\"figures/gg_pval.png\" style=\"width: 80%\"/> </td>\n",
    "        <td> <img src=\"figures/gg_rvec.png\" style=\"width: 90%\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "While this section might have seemed like something of a detour, its usefulness will made apparent soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Deriving $f_W$ and $f_W$\n",
    "\n",
    "The first step to characterize the NTS distribution is to understand the distribution of a truncated bivariate normal distrubion (TBVN). \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X_1, X_2) &\\sim \\text{TBVN}(\\theta, \\Sigma, \\rho, a, b) \\\\\n",
    "E([X_1,X_2]) &= [\\theta_1, \\theta_2] \\\\\n",
    "V([X_1,X_2]) &= \\begin{pmatrix} \\sigma_1^2 & \\rho \\sigma_1\\sigma_2 \\\\ \\rho \\sigma_1\\sigma_2 & \\sigma_2^2  \\end{pmatrix} \\\\\n",
    "X_1 &\\in \\mathbb{R}, \\hspace{3mm} X_2 \\in [a,b]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Notice that the TBVN takes the same formulation as a bivariate normal (a mean vector, a covariance matrix, and a correlation coefficient $\\rho$) except that the random variable $X_2$ term is bound between $a$ and $b$. Arnold showed that the marginal density of the non-truncated random variable $X_1$ could be written as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_W(x) &= f_{X_1}(x) = \\frac{\\phi(m_1(x)) \\Bigg[ \\Phi\\Big(\\frac{\\beta-\\rho \\cdot m_1(x)}{\\sqrt{1-\\rho^2}}\\Big) - \\Phi\\Big(\\frac{\\alpha-\\rho \\cdot m_1(x)}{\\sqrt{1-\\rho^2}}\\Big) \\Bigg] }{\\sigma_1\\cdot Z} \\tag{3} \\\\ \n",
    "m_1(x) &= (x - \\theta_1)/\\sigma_1  \\\\\n",
    "\\alpha &= \\frac{a-\\theta_2}{\\sigma_2}, \\hspace{3mm} \\beta = \\frac{b-\\theta_2}{\\sigma_2}  \\\\\n",
    "Z &= \\Phi(\\beta) - \\Phi(\\alpha)\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:arnold}\n",
    "\n",
    "Kim showed that a NTS could be written as a TBVN by a simple change of variables:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X_1 &= Z_1 + Z_2^u \\\\ \n",
    "X_2 &= Z_2^u, \\hspace{3mm} X_2 \\in [a,b] \\\\\n",
    "Z_2^u &\\sim N(\\mu_2,\\tau_2^2) \\\\\n",
    "\\theta &= [\\mu_1 + \\mu_2, \\mu_2] \\\\\n",
    "\\Sigma &= \\begin{pmatrix} \\tau_1^2 + \\tau_2^2 & \\rho \\sigma_1\\sigma_2 \\\\ \\rho \\sigma_1\\sigma_2 & \\tau_2^2  \\end{pmatrix} \\\\\n",
    "Z_1 + Z_2 &= W \\sim NTS(\\theta(\\mu),\\Sigma(\\tau^2), a, b)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $Z_2^u$ is the non-truncated version of $Z_2$. Clearly this specification of the TBVN is equivalent to the NTS and the marginal distribution of $X_1$ is equivalent to the PDF of $W$. Kim showed that the integral of $\\eqref{eq:arnold}$ was equivalent to solving the orthant probabilities of a standard BVN : \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "F_W &= F_{X_1}(x) = 1 - \\frac{L(m_1(x),\\alpha,\\rho) - L(m_1(x),\\beta,\\rho)}{Z} \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\\label{eq:kim}\n",
    "\n",
    "Hence, the PDF for a NTS can be solved analytically, and CDF can be solved using either numerical methods for the exact univariate integral in $\\eqref{eq:sheppard}$ or Cox's approximation method in $\\eqref{eq:cox}$. The code block below provides the python code necessary to calculate $\\eqref{eq:arnold}$ and $\\eqref{eq:kim}$. Although I have not implemented an efficient solution, the `NTS` class provides a quantile function $F^{-1}_W(p)=\\inf_w F_W(w) \\geq p$ that relies on [Golden's method](https://en.wikipedia.org/wiki/Golden-section_search) for univariate optimization by querying the `cdf` attribute until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTS():\n",
    "    def __init__(self, mu, tau, a, b):\n",
    "        \"\"\"\n",
    "        mu: array of means\n",
    "        tau: array of standard errors\n",
    "        rho: correlation coefficient\n",
    "        \"\"\"\n",
    "        assert mu.shape[0]==tau.shape[0]==2\n",
    "        # Assign parameters\n",
    "        self.mu, self.tau = mu.flatten(), tau.flatten()\n",
    "        self.a, self.b = a, b\n",
    "        # Truncated normal (Z2)\n",
    "        self.alpha = (self.a - self.mu[1]) / self.tau[1]\n",
    "        self.beta = (self.b - self.mu[1]) / self.tau[1]\n",
    "        self.Z = norm.cdf(self.beta) - norm.cdf(self.alpha)\n",
    "        self.Q = norm.pdf(self.alpha) - norm.pdf(self.beta)\n",
    "        # Average will be unweighted combination of the two distributions\n",
    "        self.mu_W = self.mu[0] + self.mu[1] + self.tau[1]*self.Q/self.Z\n",
    "        # Distributions\n",
    "        self.dist_X1 = norm(loc=self.mu[0], scale=self.tau[0])\n",
    "        self.dist_X2 = truncnorm(a=self.alpha, b=self.beta, loc=self.mu[0], scale=self.tau[1])\n",
    "        # W\n",
    "        self.theta1 = self.mu.sum()\n",
    "        self.theta2 = self.mu[1]\n",
    "        self.sigma1 = np.sqrt(np.sum(self.tau**2))\n",
    "        self.sigma2 = self.tau[1]\n",
    "        self.rho = self.sigma2/self.sigma1\n",
    "\n",
    "    def pdf(self, x):\n",
    "        term1 = self.sigma1 * self.Z\n",
    "        m1 = (x - self.theta1) / self.sigma1\n",
    "        term2 = (self.beta-self.rho*m1)/np.sqrt(1-self.rho**2)\n",
    "        term3 = (self.alpha-self.rho*m1)/np.sqrt(1-self.rho**2)\n",
    "        f = norm.pdf(m1)*(norm.cdf(term2) - norm.cdf(term3)) / term1\n",
    "        return f\n",
    "\n",
    "    def cdf(self, x, method='scipy'):\n",
    "        if isinstance(x, list):\n",
    "            x = np.array(x)\n",
    "        if isinstance(x, float) or isinstance(x, int):\n",
    "            x = np.array([x])\n",
    "        nx = len(x)\n",
    "        m1 = (x - self.theta1) / self.sigma1\n",
    "        bvn = BVN(mu=[0,0],sigma=[1,1],rho=self.rho)\n",
    "        orthant1 = bvn.orthant(m1,np.repeat(self.alpha,nx),method=method)\n",
    "        orthant2 = bvn.orthant(m1,np.repeat(self.beta,nx),method=method)\n",
    "        return 1 - (orthant1 - orthant2)/self.Z\n",
    "\n",
    "    def ppf(self, p):\n",
    "        if isinstance(p, list):\n",
    "            p = np.array(p)\n",
    "        if isinstance(p, float) or isinstance(p, int):\n",
    "            p = np.array([p])\n",
    "        # Set up reasonable lower bounds\n",
    "        lb = self.mu_W - self.sigma1*4\n",
    "        ub = self.mu_W + self.sigma1*4\n",
    "        w = np.repeat(np.NaN, len(p))\n",
    "        for i, px in enumerate(p):\n",
    "            tmp = float(minimize_scalar(fun=lambda w: (self.cdf(w)-px)**2,method='bounded',bounds=(lb,ub)).x)\n",
    "            w[i] = tmp\n",
    "        return w\n",
    "\n",
    "    def rvs(self, n, seed=1234):\n",
    "        r1 = self.dist_X1.rvs(size=n,random_state=seed)\n",
    "        r2 = self.dist_X2.rvs(size=n,random_state=seed)\n",
    "        return r1 + r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate NTS data with the following parameters: $\\mu_1=1$, $\\tau_1=1$ (for $Z_1$), $\\mu_2=1$, $\\tau_2=2$, $a=-1$, and $b=4$ (for $Z_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       method        mu\n",
      "0   Empirical  2.439796\n",
      "1      Theory  2.438245\n",
      "2  Quadrature  2.438245\n"
     ]
    }
   ],
   "source": [
    "# Demonstrated with example\n",
    "mu1, tau1 = 1, 1\n",
    "mu2, tau2, a, b = 1, 2, -1, 4\n",
    "mu, tau = np.array([mu1, mu2]), np.array([tau1,tau2])**2\n",
    "dist_NTS = NTS(mu=mu,tau=tau, a=a, b=b)\n",
    "n_iter = 100000\n",
    "W_sim = dist_NTS.rvs(n=n_iter,seed=1)\n",
    "mu_sim, mu_theory = W_sim.mean(),dist_NTS.mu_W\n",
    "xx = np.linspace(-5*mu.sum(),5*mu.sum(),n_iter)\n",
    "mu_quad = np.sum(xx*dist_NTS.pdf(xx)*(xx[1]-xx[0]))\n",
    "methods = ['Empirical','Theory', 'Quadrature']\n",
    "mus = [mu_sim, mu_theory, mu_quad]\n",
    "# Compare\n",
    "print(pd.DataFrame({'method':methods,'mu':mus}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above compares the empirical mean of the simulated data with the sum of the two location parameters of $Z_1$ and $Z_2$, as well as what we would estimate using a quadrature procedure with equation $\\eqref{eq:arnold}$, as $E(W)=\\int w F_W(w) dw$. Next, we can compare the actual and theoretical percentiles and quantiles against the empirical ones seen from `W_sim`. As expected, they are visually indistinguishable from each other.[[^2]]\n",
    "\n",
    "<center><h4>Figure 2: NTS P-P & Q-Q plot </h4></center>\n",
    "<img src=\"figures/gg_ppqq.png\" style=\"width: 70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.A) Application: Quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.B) Two-stage testing\n",
    "\n",
    "In a [previous post](http://www.erikdrysdale.com/regression_trial/) I discussed a two-stage testing strategy designed to validate a machine learning regression model. The framework can be generalized as follows: 1) estimate an upper-bound on the mean of a Gaussian, 2) use this upper bound as the null hypothesis on a new sample of data.[[^3]] Several simplifying assumptions need to be made including that the data are iid and from the same normal distribution: $S, T \\overset{iid}{\\sim} N(\\delta,\\sigma^2)$, and that $n$ and $m$ are large enough so that the difference between the normal and student-t distributions are sufficiently small. The statistical pipeline is as follows.\n",
    "\n",
    "1. Estimate the mean and variance on the first Gaussian sample: $(S_{1},\\dots,S_{n}) \\sim N(\\hat{\\delta}_S,\\hat{\\sigma}^2/n)$\n",
    "2. Estimate the $1-\\gamma$ quantile of this distribution: $\\hat{\\delta}_0=\\hat{\\delta}_S+n^{-1/2}\\cdot\\hat{\\sigma}\\cdot\\Phi^{-1}_{1-\\gamma}$ to set the null hypothesis:\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_0&: \\delta \\geq \\hat{\\delta}_0 \\\\\n",
    "H_A&: \\delta < \\hat{\\delta}_0 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "3. Estimate mean and variance on second Gaussian sample: $(T_{1},\\dots,T_{m}) \\sim N(\\hat{\\delta}_T,\\hat{\\sigma}_m^2)$\n",
    "4. Calculate a one-sided test statistic: $\\hat{s} = \\sqrt{m} \\cdot (\\hat{\\delta}_T - \\hat{\\delta}_0) / \\hat{\\sigma}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Reject }H_0&: \\hat{s} < t_\\alpha\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Clearly $E(s)<0$ if $\\tau >0.5$. The statistical information provided by this procedure is two-fold. First, $\\delta_0$ has the property that it will be larger than the true mean $(1-\\tau)$% of the time (this is only in the frequentist sense, so it says nothing about any realization $\\hat{\\delta}_0$ itself). Second, when the null is false (i.e. the bound holds) we will be able to compute the power in advance.  Now, where does the NTS fit into this? In order to bound the type-I error rate to $\\alpha$, we need to know the distribution of $s$ when the null is true: $\\delta > \\hat{\\delta}_0$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{s} &= \\frac{\\hat{\\delta}_T - \\delta}{\\hat{\\sigma}_m} - \\frac{\\hat{\\delta}_0 - \\delta}{\\hat{\\sigma}_m} \\hspace{2mm} \\Big| \\hspace{2mm} \\hat{\\delta}_0 > \\delta  \\\\\n",
    "&= N(0,1) - \\frac{\\hat{\\delta}_S-\\delta + \\hat{\\sigma}_n\\cdot\\Phi^{-1}_{1-\\gamma}}{\\hat{\\sigma}_m} \\hspace{2mm} \\Big| \\hspace{2mm} \\hat{\\delta}_S + \\hat{\\sigma}_n\\cdot\\Phi^{-1}_{1-\\gamma} - \\delta > 0 \\\\\n",
    "&= N(0,1) + TN\\big( - \\sqrt{m/n} \\cdot \\Phi^{-1}_{1-\\gamma}, m/n, 0, \\infty  \\big)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence the critical value $t_\\alpha = F^{-1}_W(\\alpha)$ can be found by inverting $\\eqref{eq:kim}$ (i.e. the quantile function) with the following parameters from our original notation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu &= \\big[ 0, - \\sqrt{m/n} \\cdot \\Phi^{-1}_{1-\\gamma}\\big] \\\\\n",
    "\\tau^2 &= \\big[1, m/n \\big] \\\\\n",
    "\\hat{s} | H_0 &\\sim NTS(\\theta(\\mu), \\Sigma(\\tau^2), 0, \\infty ) \\\\ \n",
    "\\hat{s} | H_A &\\sim NTS(\\theta(\\mu), \\Sigma(\\tau^2), -\\infty, 0 )\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the distribution of the test statistic only depends on $m$, $n$, and $\\gamma$. To the extent that the sample size is fixed, then the researcher can only control $\\gamma$, with a higher value increasing the power of a test, but lowering the statistical information that it provides (because the upper bound is larger). The simulations below will draw $S, T \\sim N(2,4)$, with $n=250$, ad $m=500$. We'll look at whether the `NTS` class we've created accurately captures the distribution of $\\hat{s}$ when the null is either true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta, sigma2 = 2, 4\n",
    "n, m = 250, 500\n",
    "tau, alpha = 0.1, 0.05\n",
    "mu_2stage = np.array([0, -np.sqrt(m/n)*norm.ppf(1-tau)])\n",
    "tau_2stage = np.sqrt([1, m/n])\n",
    "dist_2s_H0 = NTS(mu=mu_2stage,tau=tau_2stage, a=0, b=np.infty)\n",
    "dist_2s_HA = NTS(mu=mu_2stage,tau=tau_2stage, a=-np.infty, b=0)\n",
    "crit_val = dist_2s_H0.ppf(alpha)[0]\n",
    "\n",
    "# Compare to simulation\n",
    "nsim = 50000\n",
    "np.random.seed(nsim)\n",
    "S = delta+np.sqrt(sigma2)*np.random.randn(nsim,n)\n",
    "T = delta+np.sqrt(sigma2)*np.random.randn(nsim,m)\n",
    "delta1, delta2 = S.mean(1), T.mean(1)\n",
    "sigS, sigT = S.std(1,ddof=1), T.std(1,ddof=1)\n",
    "del S, T\n",
    "delta0 = delta1 + (sigS/np.sqrt(n))*norm.ppf(1-tau)\n",
    "shat = (delta2 - delta0)/(sigT/np.sqrt(m))\n",
    "p_seq = np.arange(0.01,1,0.01)\n",
    "s_null_H0 = shat[delta > delta0]\n",
    "s_null_HA = shat[delta < delta0]\n",
    "qq_emp_H0 = np.quantile(s_null_H0,p_seq)\n",
    "qq_emp_HA = np.quantile(s_null_HA,p_seq)\n",
    "qq_theory_H0 = dist_2s_H0.ppf(p_seq)\n",
    "qq_theory_HA = dist_2s_H0.ppf(p_seq)\n",
    "tmp1 = pd.DataFrame({'pp':p_seq,'emp':qq_emp_H0,'theory':qq_emp_H0,'Null':'H0'})\n",
    "tmp2 = pd.DataFrame({'pp':p_seq,'emp':qq_emp_HA,'theory':qq_emp_HA,'Null':'HA'})\n",
    "dat_2stage = pd.concat([tmp1, tmp2]).melt(['pp','Null'],None,'tt','qq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h4>Figure 3: Two-stage test statistic </h4></center>\n",
    "<img src=\"figures/gg_2stage_alpha.png\" style=\"width: 70%\"/>\n",
    "\n",
    "Figure 3 shows that the empirical and theoretical quantiles line up exactly as expected, showing that our NTS distribution is working with $\\mu(m,n,\\gamma)$ and $\\tau(m,n)$. We can also explore how the power changes when $m$, $n$, and $\\gamma$ are varied. In the results below, $m+n=750$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.C) Application: Data carving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "* * *\n",
    "\n",
    "### Footnotes\n",
    "\n",
    "[^1]: A standard BVN means that the means are zero, and covariance matrix has a value of one on the diagonals and $\\rho$ on the off-diagonals.\n",
    "\n",
    "[^2]: In reality, there are slight differences, they just cannot be seen on the figure.\n",
    "\n",
    "[^3]: A lower bound can be studied just as easily as an upper bond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
